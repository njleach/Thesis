\begin{savequote}[8cm]
    Quote
      \qauthor{--- author}
\end{savequote}
    
\chapter{\label{ch3}Forecast-based attribution: perturbing the boundary conditions}

This chapter contains much of the conceptual description of, and motivation for, forecast-based attribution. Using the well-predicted February 2019 heatwave as a case study, I carry out forecasts with the operational medium-range ECMWF model in which I have instantaneously perturbed the CO$_2$ concentration at initialisation. These perturbed forecasts allow me to estimate the direct contribution of diabatic heating due to CO$_2$ to the heatwave. This partial attribution provides a proof-of-concept of the forecast-based approach, and I close with a discussussion of how I could perform a more complete estimate of anthropogenic influence on a specific extreme event in following work.
\small\paragraph{Author contributions:} This chapter is based on the the following publication \footnote{with the author contributing as follows. Conceptualisation, Data curation, Formal analysis, Investigation, Methodology, Resources, Visualisation and Writing -- original draft} \par\vspace{1em}
\formatchref{Leach, N. J., Weisheimer, A., Allen, M. R., \& Palmer, T.}{2021}{Forecast-based attribution of a winter heatwave within the limit of predictability}{Proceedings of the National Academy of Sciences}{118}{49}{}{https://doi.org/10.1073/pnas.2112087118}

\clearpage

\minitoc

\clearpage

\section{Chapter open}

\section{Abstract}

  Attribution of extreme weather events has expanded rapidly as a field over the past decade. However, deficiencies in climate model representation of key dynamical drivers of extreme events have led to some concerns over the robustness of climate model-based attribution studies. It has also been suggested that the unconditioned risk-based approach to event attribution may result in false negative results due to dynamical noise overwhelming any climate change signal. The “storyline” attribution framework, in which the impact of climate change on individual drivers of an extreme event is examined, aims to mitigate these concerns. Here we propose a novel methodology for attribution of extreme weather events using the operational ECMWF medium-range forecast model that successfully predicted the event. The use of a successful forecast ensures not only that the model is able to accurately represent the event in question; but also that the analysis is unequivocally an attribution of this specific event, rather than a mixture of multiple different events that share some characteristic. Since this attribution methodology is conditioned on the component of the event that was predictable at forecast initialisation, we show how adjusting the lead time of the forecast can flexibly set the level of conditioning desired. This flexible adjustment of the conditioning allows us to synthesise between a storyline (highly-conditioned) and a risk-based (relatively unconditioned) approach. We demonstrate this forecast-based methodology through a partial attribution of the direct radiative effect of increased CO$_2$ concentrations on the exceptional European winter heatwave of February 2019.

\section{Introduction}

  Attribution of extreme weather events is a relatively young field of research within climate science. However, it has expanded rapidly from its conceptual introduction \citep{allen_liability_2003} over the past twenty years; it now has an annual special issue in The Bulletin of the American Meteorological Society \citep{peterson_explaining_2012}. Extreme event attribution is of particular importance for communicating the impacts of climate change to the public \citep{hulme_attributing_2014,hassol_natural_2016}, since the changing frequency of extreme weather events due to climate change is an impact that is physically experienced by society. As a result of this rapid expansion, there now exists a large number of different methodologies for carrying out an event attribution \citep{herring_explaining_2021}. Many of these rely on large ensembles of climate model simulations, the credibility of which has been questioned by recent studies \citep{bellprat_attribution_2016,bellprat_towards_2019,palmer_simple_2018}. A particular issue is the dynamical response of the atmosphere to external forcing, which is highly uncertain within these models \citep{shepherd_common_2016}. As attribution studies try to provide quicker results, with an operational system a clear aim, it is vital that any such system provides trustworthy results. In this study we propose a “forecast-based” attribution methodology using medium-range weather forecasts which could provide several key advantages over traditional climate model-based approaches. Firstly, if an event is predictable within a forecasting system, we know that that system is capable of accurately representing the event. Secondly, we know that any attribution performed is unequivocally an attribution of the specific event that occurred; unlike in unconditioned climate model simulations. Finally, weather forecasts are run routinely by many different national and research centers. The models used are generally state-of-the-art and extensively verified. We propose that the attribution community could and should take advantage of the massive amount of resources that are put into these forecasts by developing methodologies that use the same type of simulation. Ideally, the experiments required for attribution with forecast models would be able to be run with little additional effort on top of the routine weather forecasts; in this way they might provide a rapid operational attribution system. We discuss these ideas further throughout the text.

  There have been several studies that propose or perform methodologies related to the forecast-based attribution demonstrated here. \citet{hoerling_anatomy_2013} used two seasonal forecast ensembles to examine the predictability of the 2011 Texas drought/heatwave within a comprehensive attribution analysis involving several different types of types of climate simulation. \citet{meredith_crucial_2015} used a triply nested convection-permitting regional forecast model to investigate the role of historical SST warming within an extreme precipitation event. They conditioned their analysis on the large-scale dynamics of the event through nudging in the outermost domain. More recently, \citet{van_garderen_methodology_2021} employed spectrally nudged simulations to assess the contribution of human influence on the climate over the 20th century on the 2003 European and 2010 Russian heatwaves. Possibly the most similar studies to the one presented here are a series of studies by Hope and colleagues \citep{hope_contributors_2015,hope_what_2016,hope_determining_2019}. They used a seasonal forecast model to assess anthropogenic CO$_2$ contributions to record-breaking heat and fire weather in Australia. Two more similar studies carried out forecast-based hurricane attribution studies \citep{reed_forecasted_2020,lackmann_hurricane_2015}. Tropical cyclones are a natural candidate for forecast-based methodologies due to the high model resolution required to represent them accurately, if at all. A final distinct, but related study is \citet{hannart_dada_2016}, which proposes the use of Data Assimilation for Detection and Attribution (DADA). They suggest that operational causal attribution statements could be made in a computationally efficient manner using the kind of data assimilation procedure carried out by weather centers (to initialise forecasts) to compute the likelihood of a particular weather event under different forcings (these would be observed and estimated pre-industrial forcings for conventional attribution). Our forecast-based framework differs from these other studies in several regards. Firstly, we use a state-of-the-art forecast model to perform the attribution analysis of the event in question; rather than to solely assess the predictability of the event. We use free-running coupled global integrations here, allowing the predictable component at initialisation to dynamically condition the ensemble; as opposed to nudging our simulations towards the dynamics of the event, using nested regional simulations, or using the highly observationally constrained output of data assimilation procedures. A final key difference is that here we present an attribution of the direct radiative effect of CO$_2$ in isolation, though we hope that our approach could be extended in the future to provide an estimate of the full anthropogenic contribution to extreme weather events as in these other studies. We argue that the relative simplicity in the validation, setup and conditioning of our simulations is desirable from an operational attribution perspective; and flexible across many different types of extreme event. 

  We begin by introducing the chosen case study, the 2019 February heatwave in Europe, describing its synoptic characteristics and formally defining the event quantitatively. We then demonstrate the predictability of the event within the ECMWF ensemble prediction system, showing that this operational weather forecast was able to capture both the dynamical and thermodynamical features of the event. In \hyperref[Ch4:experiments]{Perturbed CO$_2$ forecasts}, we outline the experiments we have performed in order to quantitatively determine the direct CO$_2$ contribution to the heatwave. We then provide quantitative results from these experiments, and finally conclude with a discussion of the strengths and potential issues of our forecast-based attribution methodology, including our proposed directions for further work.

\section{The 2019 February heatwave in Europe}\label{Ch4:heatwave}

  Between the 21st and 27th February 2019, climatologically exceptional warm temperature anomalies of 10-15 \degree C were experienced throughout Northern and Western Europe \citep{young_record-breaking_2020}, as shown in Fig. $1A$. In particular, the 25th - 27th February saw record-breaking temperatures measured at many weather stations and over wide areas of Iberia, France, the British Isles, the Netherlands, Germany and Southern Sweden, as shown in Fig. $1C$ \citep{cornes_ensemble_2018}. Fig. $1D$, comparing the regional mean maximum temperatures during the 2019 heatwave with timeseries of winter mean maximum temperatures between 1950 and 2018, illustrates just how unusual and widespread the event was. This heat was associated with a characteristic flow pattern: a narrow titled ridge extending from north-west Africa out to the southern tip of Scandinavia, advecting warm subtropical air north-east \citep{sousa_european_2018}, as shown in the geopotential height field in Fig. $1A$. This dynamical driver was accompanied by another synoptic feature that further enhanced the warming: widespread clear skies between the 25th - 27th, shown in Fig. $1B$. These clear skies resulted in a widespread and persistent strong diurnal cycle, reaching 20 \degree C in some locations. Further details of the meteorological mechanisms and historical context of the heatwave are provided in refs. \citep{young_record-breaking_2020,kendon_temperature_2020,christidis_extremely_2021}.
  
  In order to quantify the direct impact of CO$_2$ on the heatwave in question within this study, we need to characterise the heatwave in an ``event definition''. The choice of event definition is subjective but can impact on the quantitative results of an attribution study significantly \citep{leach_anthropogenic_2020,uhe_comparison_2016,kirchmeieryoung_importance_2019}. The most remarkable feature of the February 2019 heatwave were the maximum temperatures observed, which peaked between the 25\textsuperscript{th} and 27\textsuperscript{th} for the majority of the affected area. Focusing on this relatively short time-period ensures that the synoptic situation driving the heat is coherent throughout the event definition window. For the spatial extent of the event, we use the eight European sub-areas described in ref. \citep{christensen_summary_2007}. The use of regions previously defined in the literature aims to avoid selection bias. Our resulting event definition is as follows: the hottest temperature observed between 2019-02-25 and 2019-02-27, then averaged over the land points within each region (the temporal maximum is calculated before the spatial averaging). Although we carry out our calculations for all sub-areas, several regions were characteristically very similar in terms of both the event itself, and the forecasts of the event. We therefore focus on three of the eight regions: the British Isles (BI), which experienced exceptional heat and was well predicted; France (FR), which experienced exceptional heat but where the magnitude of the heat was less well forecast; and the Mediterranean (MD), which experienced well-predicted but climatologically average heat.

  \clearpage
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{{Fig3.1}.pdf}
    \caption[The 2019 February heatwave in Europe: synoptic characteristics \& historical context.]{\textbf{The 2019 February heatwave in Europe: synoptic characteristics \& historical context.}}
  \end{figure}
  \clearpage

  \subsection{Forecasts of the heatwave}\label{Ch4:forecasts}

    This heatwave was well-predicted by the European Centre for Medium-Range Weather Forecasts (ECMWF) ensemble prediction system. Their forecasts indicated ``extreme" heat was possible at a lead time of around two weeks, and probable at a lead time of around ten days (Fig. $2A$), despite the exceptional nature of the heatwave in both the model climatology and real world. As expected, the forecast's performance in predicting the extreme heat at the surface is reflected in variables more closely linked to the dynamic drivers of the heat, such as 500 hPa geopotential height (Fig. $2B$). 
    
    This successful forecast is a crucial part of our study as it means that we are not only confident that the model used is able to simulate the event in question; but that we are unequivocally performing an attribution analysis of the specific winter heatwave that occurred in Europe during February 2019. This is an important distinction to the framework used in ``conventional'' or ``risk-based'' \citep{shepherd_common_2016} attribution studies \citep{stott_human_2004,pall_anthropogenic_2011,sparrow_attributing_2018,leach_anthropogenic_2020}, which in general reduce the event to some impact-relevant quantitative index , then estimate the increase in likelihood of events that exceed the magnitude of the event in question. For example, a heatwave attribution study may choose to define the event as the hottest observed temperature during the heatwave, and then compute the attributable change in likelihood of temperatures hotter than this recorded maximum (eg. using models or historical records). While this does provide useful information, it does not answer the question of how much more likely anthropogenic activities have made the \emph{specific} heatwave that occurred, rather the question of how much more likely anthropogenic activities have made a mixture of events that share one or more characteristics. Studies have attempted to provide a more satisfactory answer to this first question by including a level of conditioning on the set of events considered by using circulation analogues \citep{yiou_statistical_2017}, or by nudging model simulations towards the specific dynamical situation that occurred during the event in question \citep{meredith_crucial_2015,van_garderen_methodology_2021}. Here we are evidently performing an attribution study of the specific record-breaking heatwave that occurred in February 2019 due to the use of these successful forecasts, that not only captured the heat experienced at the surface, but also the dynamical drivers behind the heat.

    As well as enabling us to answer the attribution question for a single specific heatwave, the use of a numerical weather prediction model provides additional benefits. Since large model ensembles are required to properly capture the statistics of extreme events, many previous attribution studies, especially in the context of heatwaves, have used relatively coarse, atmosphere-only climate models \citep{massey_weatherhome-development_2015,ciavarella_upgrade_2018,christidis_new_2013}, which may not fully capture all the physical processes required to credibly simulate the extreme in question \citep{sillmann_understanding_2017}. In particular, the use of atmosphere-only simulations may result in the full space of climate variability being under-sampled due to the lack of atmosphere-ocean interaction \citep{fischer_biased_2018}. This can lead to studies overestimating the impact of anthropogenic activity on weather extremes \citep{leach_anthropogenic_2020,bellprat_attribution_2016}. More generally, Bellprat et al., and Palmer and Weisheimer \citep{bellprat_towards_2019,palmer_simple_2018} have shown the importance of initial-value reliability in model ensembles underlying robust attribution statements. Model evaluation is therefore a key part of any robust model-based attribution study. Here, the demonstrably successful forecast enables us to be confident that the model used is providing credible realisations of the event.

    A clear distinction between the typical climate model simulations used for attribution \citep{massey_weatherhome-development_2015,christidis_new_2013} and the forecasts used here is that the climate model simulations are usually allowed to spin out for a sufficient length of time such that they have no memory of their initial conditions; an ensemble constructed in this way will therefore be representative of the climatology of the model. If such simulations use prescribed-SST boundary conditions, then the ensemble will be representative of the climatology conditioned on the prescribed SST pattern \citep{ciavarella_upgrade_2018}. Unlike climatological simulations, a successful forecast is conditioned upon the component of the weather that is predictable at initialisation. In general, the level of conditioning imposed upon the ensemble by the initial conditions reduces as the model integrates forwards from the initialisation date. Hence a forecast ensemble initialised only a few days before an event will be much more heavily conditioned (and therefore much less spread) than one initialised weeks before. As the lead time increases, a forecast ensemble will tend towards the model climatology, analogous to the climate model simulations discussed above. We can relate these situations to the two broad attribution frameworks discussed in \citep{shepherd_common_2016}: very long lead times, where the forecast simulates model climatology, are analogous to ``conventional'' attribution; while short lead times, in which the forecast ensemble is heavily dependent on the initial conditions and therefore conditional on the actual dynamical drivers that lead to the extreme event, are analogous to the ``storyline'' approach in \citep{hazeleger_tales_2015,van_garderen_methodology_2021}. In order to synthesize between these two frameworks, here we have chosen 4 initialisation times (3-, 9-, 15-, and 22-day leads) for our experiments that span the range from a near-unconditioned climatological forecast to a short-term forecast that is tightly conditioned on the actual dynamical drivers of the heatwave.

    \clearpage
    \begin{figure}[h]
      \centering
      \includegraphics[width=\textwidth]{{Fig3.2}.pdf}
      \caption[Medium- to extended-range forecasts of the heatwave.]{\textbf{Medium- to extended-range forecasts of the heatwave.}}
    \end{figure}
    \clearpage

\section{Perturbed CO$_2$ forecasts}\label{Ch4:experiments}

  In this study we choose to only change one feature of the operational forecast in our experiments: the CO$_2$ concentration. This means that the analysis we carry out is limited to attributing the impact of diabatic heating due to increased CO$_2$ concentrations above pre-industrial levels just over the days between the model initialisation date and the event. Although this results in a counterfactual that does not correspond to any ``real'' world (since it is one with approximately present-day temperatures but pre-industrial CO$_2$ concentrations), and thus reduces the relevance of our analysis to stakeholders or policymakers; it does significantly increase the interpretability of our results, and remove a major source of uncertainty associated with a “complete” attribution to human influence: the estimation of the pre-industrial ocean and sea-ice state vector used to initialise the model \citep{stone_benchmark_2021}. Here we define a complete attribution as an estimate of the total impact of human influence on the climate arising from anthropogenic emissions of greenhouse gases and aerosols since the pre-industrial period. For each lead time chosen, in addition to the operational forecast (ENS) we run two experiments using operational initial conditions and identical to the operational forecast in every way except the experiments have specified fixed CO$_2$ concentrations. One experiment has CO$_2$ concentrations fixed at pre-industrial levels of 285 ppm (PI-CO$_2$), while in the other they are increased to 600 ppm (INCR-CO$_2$). These represent approximately equal and opposite perturbations on global radiative forcing \citep{etminan_radiative_2016}. We carry out these two experiments for each lead time, perturbing the CO$_2$ concentration in opposite directions, to ensure that any changes to the likelihood of the event can be confidently attributed to the changed CO$_2$ concentrations. It is possible that, due to the chaotic nature of the weather, the operational conditions were ideal for generating the observed extreme, and any perturbation to the dynamical system would reduce the likelihood of its occurrence \citep{shepherd_common_2016}. If this were the case we would see a reduction in event probability regardless of whether we increased or reduced the CO$_2$ concentration.
  
  Some previous work has been done on the impact of reduced CO$_2$ concentrations in the absence of changes to global sea surface temperatures. Baker et al. \citep{baker_higher_2018} explored how temperature and precipitation extremes were affected by the direct effect of CO$_2$ concentrations (defined there as all the effects of CO$_2$ on climate beside those occurring through ocean warming), finding the direct effect of CO$_2$ increases risk of temperature extremes, especially within the Northern hemisphere summer. Our experimental design is also reminiscent of some of the earliest work done on investigating the impact of CO$_2$ on climate in global circulation models \citep{gates_preliminary_1981,mitchell_seasonal_1983}. This work found that, in the absence of changes to sea surface temperatures or sea ice concentrations, a doubling of CO$_2$ concentrations would change global mean surface temperatures over land by $\sim0.4$ \degree C. These early studies indicate that changes in global land temperatures are approximately linear with the logarithm of CO$_2$ concentration.
  
  We find that the best-estimate global mean change in land surface temperatures attributable to the additional diabatic heating due to CO$_2$ over pre-industrial levels (henceforth the ``CO$_2$ signal'', calculated as half the difference between the two experiments for a particular variable) at a lead time of two weeks (over the final 5 days of the forecasts initialised on 2019-02-11) is 0.22 [0.20 , 0.25] \degree C (square brackets indicate a 90 \% confidence interval throughout). In general, the further away from the initialisation date, the slower the rate of change of the globally-averaged ensemble mean CO$_2$ signal, and the larger the ensemble spread (Fig. $3A$). While in experiments with prescribed SSTs, we might expect the CO$_2$ signal in surface temperatures to approach a maximum value within timescales on the order of months, in our experiments the CO$_2$ signal will likely continue to increase in magnitude for centuries due to the ocean-coupling, as is the case in the abrupt-4xCO$_2$ experiment carried out in CMIP \citep{taylor_overview_2012,eyring_overview_2016,rugenstein_equilibrium_2020}. The zonal-mean patterns of surface temperature CO$_2$ signal are qualitatively similar to those exhibited by CMIP5 and CMIP6 models during the abrupt-4xCO$_2$ experiment \citep{flynn_climate_2020,andrews_dependence_2015}, despite the considerably shorter timescales involved: small and very confident changes in the tropics become larger but much less confident changes at the poles. This heterogeneity in the zonal distribution of warming appears to originate in the zonal distribution of the lapse-rate feedback; the weekly timescales of these experiments is insufficient for the surface-albedo feedbacks to have any significant impact \citep{smith_polar_2019}.
  
  We also examine the impact on the specific event dynamics over our region of interest; since these were crucial in developing the extremes observed. Fig. $3B$ shows the growth in Z500 errors (measured as the mean absolute distance from ERA5 over the European domain) for each of the experiments. This figure illustrates that there are no clear differences in the ability of each experimental ensemble to predict the dynamical characteristics of the event. In other words, we have not made the synoptic event any more or less likely as a result of our perturbations. This is crucial as it means that we can consider any changes to the magnitude of the temperatures observed to be entirely due to the thermodynamic effect of changed diabatic CO$_2$ heating, and not due to the attractor of the dynamical system having changed as a result of the perturbations we have made.
  
  Figs. $3C$ and $3D$ show analogous plots to $3B$, but for inter-experimental and intra-ensemble errors respectively. These indicate a couple of important features. Firstly, no two experiments are more similar than any other two; the magnitude of Z500 distances in Fig. $3C$ are near identical for all lead and validation times. Secondly, the error growth due to the CO$_2$ perturbation is slower than due to the initial condition perturbations; the errors in Fig. $3C$ increase slower than in $3D$. However, by the end of the longest lead forecast, we can see that the intra-ensemble errors have saturated, and the inter-experimental errors have grown to be the same magnitude. The saturation of intra-ensemble errors by the end of this lead time reinforces our assertion that at this lead the forecast is a good approximation of a climatological simulation.

  \clearpage
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{{Fig3.3}.pdf}
    \caption[Global temperature and synoptic-scale dynamical response to CO$_2$ perturbations.]{\textbf{Global temperature and synoptic-scale dynamical response to CO$_2$ perturbations.}}
  \end{figure}
  \clearpage

\section{Attributing the heatwave to diabatic CO$_2$ heating}\label{Ch4:attribution}

  First, we examine the geographical pattern of the CO$_2$ signal in the heatwave in Fig. $4A-D$. These indicate several key features of the attributable direct CO$_2$ effect on the heatwave. The CO$_2$ effect tends to grow with lead time, consistent with its impact on global mean temperatures. It is generally stronger over land than ocean, also consistent with global mean temperatures. Finally, the ensemble tends to become less confident in its effect as the lead time increases and the ensemble members diverge. The CO$_2$ signal magnitude in the heatwave generally exceeds the signal in the global mean surface temperature (Fig. $3A$), in particular in Central Europe; possibly due to the high contribution of diabatic heating to the heatwave arising from ideal dynamical conditions. Fig. $4E$ shows boxplots of the heatwave CO$_2$ signal for the three regions of interest plus the full European land area. Although there is some region-specific variability, these reinforce the main messages illustrated by the maps: the CO$_2$ signal grows and decreases in confidence as the lead time increases.

  In addition to the absolute impact of the direct CO$_2$ effect on the heatwave, we also carry out a probabilistic assessment of its impact, consistent with conventional ``risk-based'' attribution studies \citep{shepherd_common_2016,winsberg_severe_2020}. Due to the novel approach we are taking within this study, it is worth clarifying exactly what question we are answering with this probabilistic analysis. The specific question is: ``given the forecast initial conditions, how did the direct impact of increased CO$_2$ concentrations compared to pre-industrial levels just over the days between initialisation and the heatwave itself change the probability of temperatures at least as hot as were observed?''. Using conventional attribution terminology, we call the operational forecast ensemble of the event as our ``factual'' ensemble, and the pre-industrial CO$_2$ experiment as our ``counter-factual'' ensemble. We calculate the probability of simulating an event at least as extreme as observed in the factual ensemble, $P_1$, and in the counterfactual ensemble, $P_0$. These probabilities are estimated by fitting a generalised extreme value distribution to the 51-member ensemble in each case. We then express the change in event probability as a risk-ratio, $RR=P_1/P_0$, which represents the fractional increase in the likelihood of an event at least as extreme as observed in the factual ensemble over the counterfactual ensemble \citep{stott_human_2004,stone_end--end_2005}. Uncertainties are estimated with a 100,000 member bootstrap with replacement, rejecting samples for which the probability of the event in the factual ensemble is zero. The resulting risk-ratios are shown in Fig. $4F$. There are several key factors that contribute to the best-estimate and confidence in the risk ratios: the CO$_2$ signal growth with lead time; the ensemble spread growth with lead time; how extreme the event was; and how well-forecast the event was. The larger the CO$_2$ signal, the greater the increase in risk; the larger the ensemble spread, the lesser the increase in risk and the lower the confidence; the more extreme the event, the greater the increase in risk; and the better the forecast (ie. the closer the event to the ensemble centre), the greater the confidence. 
  
  We find that on the shortest lead time, the direct CO$_2$ effect increases the probability of the event over all European regions (significant at the 5 \% level based on a one-sided test). For the well-forecast event experienced over the British Isles, the direct CO$_2$ effect increases the probability of the extreme heat by 42 [30 , 60] \%. For the France heatwave, which was well-forecast given its exceptional nature, but for which the ensemble did not quite reach the total magnitude of the heat experienced, the event probability increased by at least 100 \% (5\textsuperscript{th} percentile), but with a very wide uncertainty range. Finally, for the least remarkable but relatively well-forecast event over the Mediterranean, the direct impact of CO$_2$ increased the event probability by 6.7 [4.6 , 9.7] \%. These results from the very short lead experiments represent very highly conditioned statements: in both ensembles the dynamical evolution of the event was near-identical (pattern correlation of $>0.99$ for all ensemble members, Fig. $2B$).
  
  Moving out to the longer lead times, we find that the confidence in the change in event probability decreases almost ubiquitously. This is as expected, since the further we move away from the event, the less highly conditioned our ensemble is, and the more dynamical noise we are adding to the system \citep{shepherd_common_2016}. However, for the 9-day lead forecast, the uncertainty is low enough to have confidence in the results for the majority of study regions. In particular, the British Isles heatwave, for which the 9-day lead forecast was better than several of the regional 3-day lead forecasts (as measured by the Continuous Ranked Probability Skill Score), increases in probability by 52 [29 , 94] \% due to the direct CO$_2$ effect. However, for France the uncertainty range is so large that based on these results alone we would have no confidence in the direction of the CO$_2$ effect. Moving further out to the 15- and 22-day lead forecasts, this loss in confidence becomes more pronounced, especially for the British Isles region. For this region, we can get virtually no useful information out of these probabilistic results for the two longest lead experiments. This drop-off in confidence arises due to the increasing ensemble spread from dynamical noise, and large reduction in the number of factual ensemble members able to simulate an event as hot as occurred in reality between the 9- and 15-day leads. A similar, though generally less pronounced drop-off in confidence is found in all other regions. 
  
  We can make use of our INCR-CO$_2$ experiment to increase our confidence that the positive results we obtained in the probabilistic analysis above are in fact due to the direct CO$_2$ effect, and not just random variability. If CO$_2$ were driving the changes in event probability between the PI-CO$_2$ and operational forecasts, then we would expect to see an even more dramatic increase in event probability between the PI-CO$_2$ and INCR-CO$_2$ forecasts. This is indeed what we find. For all regions and lead times, our best-estimate change in event probability is above zero when CO$_2$ concentration is increased from pre-industrial levels of 285 ppm to 600 ppm. This therefore increases our confidence further that the positive attribution to CO$_2$ under high conditioning is genuinely significant. From these results, it also appears that there is a general trend of change in event probability increasing as the forecast lead increases, similar to the absolute impact of the direct CO$_2$ effect trend; though it is still somewhat masked by uncertainty.
  
  An important caveat on all of these results, probabilistic and absolute, is that they represent a lower bound on the estimate of the direct CO$_2$ effect. As is clear from the development of the CO$_2$ signal estimates with lead time, the model is still adjusting to the sudden change in CO$_2$ concentration (and would continue to do so for centuries due to the very long deep ocean equilibration timescales). Hence we would expect the ``full'' effect of CO$_2$ to be greater than the estimates we present here. This is consistent with a recent study that used unconditioned climate model simulations to carry out an attribution of the complete anthropogenic contribution to the same event, which produced much higher estimates of the risk-ratio \citep{christidis_extremely_2021}.

  \clearpage
  \begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{{Fig3.4}.pdf}
    \caption[Attribution of the direct CO$_2$ influence on the heatwave.]{\textbf{Attribution of the direct CO$_2$ influence on the heatwave.}}
  \end{figure}
  \clearpage

\section{Discussion}\label{Ch4:discussion}

  Here we have presented a partial, forecast-based attribution of the European 2019 winter heatwave. Taking advantage of successful medium-range forecasts from ECMWF, we used a state-of-the-art numerical weather prediction model that was demonstrably able to predict the event to attribute the direct impact of CO$_2$ through diabatic heating over pre-industrial levels and just over the days immediately preceding the event on the high temperatures experienced in several regions of Europe. We explored how the level of dynamical conditioning imposed can be specified by changing the lead time of the forecasts. Finally, we presented our quantitative results using two different approaches: measuring the attributable absolute and probabilistic impacts of CO$_2$; inspired by the ``storyline'' and ``risk-based'' attribution frameworks \citep{stott_human_2004,shepherd_common_2016,winsberg_severe_2020,jezequel_behind_2018}.  
  
  There are several advantages associated with this novel forecast-based attribution methodology, compared to conventional climate model based attribution. One simple advantage is that forecast models generally represent the technological peak within the spectrum of General Circulation Models. They tend to have a higher resolution than the models used for climate simulation. In addition, the forecast model used here is coupled, while the large climate model ensembles used for attribution tend to use prescribed sea surface temperatures \citep{ciavarella_upgrade_2018}. The use of prescribed SSTs can lead to model biases that project strongly onto attribution results \citep{fischer_biased_2018}. A final advantage arising from the use of an operational forecast model is the wealth of literature and model analysis that will already be available before an attribution study is initiated. As well as these advantages associated with the type of model there is the crucial advantage associated with using successful forecasts: the specific and intrinsic model verification. Due to the difficulty in fully quantifying how well climate models can represent an individual specific event (in particular, the very large ensembles required to have a large enough sample of characteristically similar events), climate model based attribution studies tend to perform statistical model evaluations; or/and account for this uncertainty through multi-model ensembles \citep{philip_protocol_2020}. On the other hand, if a forecast model that demonstrably predicted the event as it occurred is used, no further model verification or evaluation is required to test whether the model is capable of producing a faithful representation of the specific event.
  
  Related to this intrinsic verification is an important point on the framing of forecast-based attribution studies. Climate model based attribution studies tend to characterise an event in terms of some quantitative index closely related to the impact of the event (such as the maximum temperature observed during a heatwave). They then use climate model simulations to determine how climate change has affected the probability of observing an event at least as extreme as the actual event. This is often done without imposing any dynamical conditioning on the simulations, though this is an area of active research \citep{yiou_statistical_2017,pall_diagnosing_2017}. This unconditional approach means that the specific question being answered is not ``how has anthropogenic climate change affected the probability of event X?'', but ``how has anthropogenic climate change affected the probability of all events that are at least as extreme as event X in terms of the index used to define X?''.  This second question does not fully answer the question of how climate change has affected the actual event that the study is concerned with. In contrast, the use of a forecast model that predicted the event ensures that any attribution analysis is unequivocally an attribution of that specific event \citep{hope_determining_2019}.
  
  In addition to its advantages, this novel forecast-based attribution methodology also has associated issues that must be overcome. Firstly, the forecast model must have produced a ``good'' forecast of the event. If the model is unable to represent the event as it happened, then we cannot have confidence in any estimates of the impact of climate change on that event. Issues can arise even in qualitatively ``good'' forecasts, such as the forecast of the heatwave over France in this study. As very few ensemble members, if any, exceeded the observed magnitude of the event for this region, the confidence in our estimates of the probabilistic impact of CO$_2$ on the event is extremely low (since we are extrapolating the distribution shape outside of the range of our data). Although the estimates of the absolute impact of CO$_2$ do not share this lack of confidence, this is still a problem. It is possible that applying some bias correction procedure \citep[e.g.][]{sippel_novel_2016,jeon_quantile-based_2016,li_reducing_2019} based on the model climatology to the model output before analysis might alleviate these issues to some extent, but not if the model is simply unable to predict the event in question (ie. a forecast bust). Secondly, the short timescales involved in these medium-range forecasts mean that the interpretation of any results becomes more difficult as the model is still adjusting to the perturbations imposed \citep{hope_contributors_2015}, at least in the case of the CO$_2$ perturbations applied here. This adjustment is clear on a global scale in Fig. $3A$. Due to this incomplete adjustment, any quantitative statements of attribution represent a lower bound on the ``true'' value. 
  
  We have shown that the direct effect of CO$_2$ concentrations over pre-industrial levels on the February heatwave is significant, even on timescales as short as a few days. Based on the very good 9-day lead forecast of the heatwave over the British Isles, the region that saw the most climatologically exceptional event, the direct effect of CO$_2$ was to increase the magnitude of the heatwave by 0.31 [0.24 , 0.37] K, and the conditional probability of the heatwave by 52 [29 , 94] \%. It is very important to bear in mind that this statement of risk is highly dynamically conditioned (Fig. $2B$). These estimates of the impact of CO$_2$ on the heatwave follow the storyline attribution framework, since we have effectively removed the dynamical uncertainty from our simulations with this strong conditioning imposed by the short lead time \citep{shepherd_common_2016,shepherd_storylines_2018,jezequel_behind_2018}. Our longer, 22-day lead experiments can contrast this storyline analysis with relatively unconditioned results much closer to the climatological simulations typically used in the conventional ``Risk-based'' attribution framework \citep{philip_protocol_2020,stott_human_2004}. At this lead, we find that although over all regions the best-estimate impact of the direct CO$_2$ effect is to enhance the heatwave by approximately 0.5 K, in none of the regions is this impact significantly positive at the 90 \% level (based on the bootstrapped confidence in the median value). Corresponding estimates of the risk ratio have so low confidence that they provide virtually no useful information. Increasing the forecast ensemble size, which is small compared to the climate model ensembles used in most attribution studies, would increase the confidence, potentially resulting in useful quantitative estimates of the risk ratio even at these longer lead times. Our results illustrate some of the concerns voiced recently over the conventional risk-based approach to attribution \citep{winsberg_severe_2020,shepherd_common_2016}. Due to the dynamical noise present in unconditioned ensembles, it is possible to obtain an inconclusive attribution within a conventional risk-based framework, and at the same time obtain a confident positive attribution if the dynamical uncertainty is removed through conditioning (in our case achieved by reducing the forecast lead).
  
  While this study provides a demonstration of the potential use for forecast models within attribution science, it remains a partial attribution to the direct CO$_2$ effect only. For forecast-based attribution to provide results that are fully comparable to conventional climate model-based attribution, we will need to demonstrate how the complete anthropogenic contribution to an extreme event could be estimated with successful forecasts. The next step to progress forecast-based attribution further will be to remove an estimate of the anthropogenic contribution to ocean temperatures from the model initial conditions \citep[e.g.][]{stone_benchmark_2021}. If performed in addition to reducing other greenhouse gas concentrations and aerosol climatology down to their pre-industrial levels, this should allow us to run pre-industrial forecasts of an event. This has been done previously for a seasonal forecast model by Hope et al. \citep{hope_contributors_2015,hope_what_2016,hope_determining_2019}. They removed the anthropogenic signal from 1960 onwards from the initial conditions, but we could in principle remove the signal from pre-industrial times onwards in order to estimate the complete anthropogenic contribution to an event. Although it is highly likely that there will be methodology specific issues that arise in this direction, we suggest that being able to estimate the complete anthropogenic contribution to an extreme event using a forecast model that was able to predict the event in question would be extremely valuable. Developing a methodology to allow us to do so might also provide a pathway to operational attribution being able to be carried out by weather prediction centres, due to the routine frequency at which they produce forecasts. In addition to attempting a ``complete'' forecast-based attribution of an extreme event, we would like to explore how increasing the ensemble size may allow us to provide confident forecast-based attribution analyses within the unconditioned risk-based framework (ie. at long forecast lead times). One potential avenue to allow us to do this efficiently might be to reduce the resolution of the forecasts, though this would not be appropriate if it reduced the ability of the model to represent the event in question. On a similar note, we would also like to extend our experiments out to seasonal timescales. This would reduce the issues with the interpretation of our medium-range results that occurred due to the model adjustment to the sudden changes to the CO$_2$ concentration. It is possible that seasonal forecasts have the greatest potential to target for an operational forecast-based attribution methodology.

\section{Chapter close}