{\onehalfspacing%
\begin{savequote}[8cm]
  Human-induced climate change is already affecting many weather and climate extremes in every region across the globe.
      \qauthor{--- IPCC, AR6, 2022}
\end{savequote}
    
\chapter{\label{discussion}Discussion} 

In this final chapter, I consider this thesis as a whole. Starting with a brief overview of everything that I have covered within the science chapters, I then move on to discussing related precedent works, including how each is similar to and different from my own, and the advantages and disadvantages of various approaches. I examine the limitations of the forecast-based approach to attribution that has been my focus --- and how these limitations could be overcome in further work. I expand this discussion of future research directions to include potential applications beyond the attribution of extreme weather. I end the chapter, and thesis, with some closing thoughts.

\clearpage

\minitoc

\clearpage}

\section{An overview of this thesis}\label{discussion:overview}

  I started this thesis with a discussion of the precise question that this thesis would be examining, posing it as: `how has human influence on the climate affected both the probability and intensity of specific extreme weather events?'. Following this, I provided several reasons why this is an interesting and useful question to answer, including: quantifying the fraction of damages from individual weather events that GHG emitters are liable for in both tort law and loss and damage frameworks; engaging the public in climate change research and impacts; improving our understanding of extreme weather to improve both adaptation planning and the models we use to simulate such weather. I then described the two most common frameworks that have previously been used to answer this question: the probabilistic and storyline approaches. Both of these approaches have advantages and disadvantages, but I suggested that they could be synthesised and complemented through the use of counterfactual weather forecasts. One way of achieving such a forecast-based approach is the main contribution of this thesis to the literature. I concluded with a description of the underlying physics behind heatwaves, and the basis for how they are being affected by human influence on the climate.

  In \hyperref[ch2]{chapter 2} I presented an attribution analysis of the 2018 heatwave in Europe. This analysis used a conventional probabilistic approach to explore whether two seemingly conflicting quantitative attribution results publicised in the media could be reconciled by digging into the technical details. I found that this conflict could indeed be largely resolved when considering the different temporal scales used to define the event in those previous results. This analysis provided an introduction to the methods used widely in probabilistic attribution. However, it also resulted in a number of questions about the approach, concerning model validation and the specificity of the analysis: in particular, whether I could genuinely claim to be attributing the specific 2018 event given that none of the meteorological context had been taken into account in the probabilistic approach followed in this chapter. One apparent solution to these questions would be to use weather forecast models that had successfully predicted the event in question to assess the human contribution to the event --- as opposed to the climate models that are commonly used. I developed this idea of forecast-based attribution in the two following chapters.

  \hyperref[ch3]{Chapter 3} presented the first example of my forecast-based approach to attribution. Motivated by the remarkably accurate forecast of an exceptional heatwave in February 2019 by the operational ECMWF ensemble prediction system, I perturbed the CO$_2$ concentration in this system back to pre-industrial levels to examine how the heatwave would be impacted. I argued that --- in addition to the high-resolution of the forecast model --- this successful prediction is a key feature of the forecast-based approach, as it largely guarantees that the model is able to represent all the key processes that drove the heatwave. An important limitation of this study is that it only addressed the contribution of a single component of anthropogenic influence on the climate: the direct radiative impact of increased CO$_2$ concentrations above pre-industrial levels. However, despite this limitation, I still found a detectable CO$_2$ signal in both the intensity and probability of the heatwave. One surprise was how rapidly the signal was detectable within the forecasts: it could be detected even at a very short lead of 3 days. Another interesting feature was how stable the predictability of the heatwave was: despite the relatively large perturbation made to the forecast boundary conditions, it remained predictable in both reduced and increased CO$_2$ counterfactual forecasts. Although there were several possible directions for future work, I identified that trying to produce a more complete estimate of human influence on an individual event was the most important one, and so the following chapter explores doing exactly that.

  The 2021 Pacific Northwest Heatwave is the case study used in \hyperref[ch4]{chapter 4}. This was a truly unprecedented event that not only shattered local temperature records by several degrees, but also presented conventional attribution methods with a considerable challenge. Statistical models often suggested that the event should have been impossible, and it was unclear if climate models were even able to simulate such an extreme event. I argued the reason for these challenges were the very specific processes that drove the heatwave: an atmospheric river coincided with an anticyclonic block, providing both diabatic and adiabatic heating to air parcels that then descended to the surface and heated further through soil-moisture feedbacks and insolation. This optimal combination of processes had likely not been seen before in the historical record --- hence the difficulty faced by statistical approaches. However, the event was incredibly well-forecast given this context, with suggestions of such extreme temperatures appearing over 10 days before they actually occurred. I took the same ECMWF model used in \hyperref[ch3]{chapter 3}, and modified both the CO$_2$ concentration boundary conditions and the initial ocean conditions to produce an initial state consistent with a climate without human influence. These perturbations resulted in a pre-industrial counterfactual forecast; and I also applied exactly opposite perturbations to produce a `future' counterfactual forecasts. These counterfactual forecasts indicated that the heatwave was made at least 8 [2--30] times more likely due to climate change --- and that this risk is doubling every 17 [10--50] years at the current rate of global warming. Although there remain scientific challenges for this approach to overcome, this chapter demonstrated that an operational attribution service based on counterfactual forecasts is not far out of reach, and could provide extremely valuable information for adaptation planning.

  \hyperref[ch5]{Chapter 5} provided a contrasting but complementary study. While extreme event attribution is typically backward-looking, examining changes in weather since the pre-industrial period, here I looked at the projection of climate extremes. And while extreme event attribution focuses in on a single specific event, here I explored how the full space of uncertainty in all such extremes could be sampled from. I performed this exploration upon a novel set of large-ensemble atmosphere-only simulations forced by lower boundary conditions from three of the most extreme winters simulated within the UKCP18 coupled global runs. These $\sim$1000-member ensembles allowed me to better understand the uncertainty surrounding these extreme winters, including to what extent they were forced or whether they were driven by atmospheric internal variability. They demonstrated that the atmosphere-only model used was not only able to simulate such extremes, but also to produce extremes beyond anything found in UKCP18 --- even beyond what might be expected from statistical extrapolation. I suggested that this computationally efficient methodology could be used to provide a rich multivariate set of spatially, temporally and physically coherent samples of extremes, and that such a set could provide valuable information for adaptation planning and designing high-impact low-likelihood scenarios of future weather. It could be complemented by specific `Tales of future weather': counterfactual weather forecasts of damaging historical events as if they occurred in a warmer world, analogous to those in \hyperref[ch4]{chapter 4}. Such Tales could provide detailed information about the future risk from and physical understanding of specific extremes, while the large ensembles in this chapter could provide broad information about the range of future extremes. I discuss this further below in the context of \hyperref[discussion:future]{future research directions}.

\section{This thesis in the context of previous work}\label{discussion:context}

  \paragraph*{First approaches using weather forecast models}

    Forecast-based attribution as an idea is not novel to this thesis, though this thesis does extend the approaches taken previously. One of the main reasons why weather forecast models have been used previously is their high resolution --- though simply using weather forecast models is not necessarily the same as using weather forecasts. Such high resolution is required for representing localised dynamical weather extremes such as hurricanes or convective storms. Possibly the first study to explicitly use a weather forecast model for the purpose of attribution was \citet{lackmann_hurricane_2015} \footnote{Although \citet{hoerling_anatomy_2013} did make use of weather forecasts in their storyline analysis of the 2011 Texas combined heatwave and drought, these were not used for attribution explicitly, but rather to explore the predictability of the event.}. \citeauthor[]{lackmann_hurricane_2015} used the Weather Research and Forecasting (WRF) model \citep{skamarock_description_2005} in a triply-nested setup driven by reanalysed SSTs to analyse how Hurricane Sandy may have evolved differently in both pre-industrial and future climates. The counterfactual climate forecasts were produced by perturbing the initial and boundary conditions in line with changes estimated from CMIP3 GCM simulations \citep{meehl_wcrp_2007}. With this experiment design, \citeauthor{lackmann_hurricane_2015} showed that Sandy's intensity would have reduced and its track would have shifted southward, conditioned on its predictable component at a lead of around 3 days. An analogous design was used by \citet{meredith_crucial_2015}, who used a similar triply-nested WRF model setup to examine the highly nonlinear influence of SST warming trends on a convective extreme near the Black Sea. \citeauthor{meredith_crucial_2015} also used perturbed SSTs for their counterfactual simulations, but did not perturb other initial or boundary conditions. They additionally nudged the large-scale circulation to keep the dynamics consistent between their simulations. 

  \paragraph*{The pseudo-global warming method}

    The approach taken by \citeauthor{meredith_crucial_2015} falls under a broad umbrella of approaches often referred to as `pseudo-global warming' experiments \citep{schar_surrogate_1996}. In the basic version of this experiment design, a regional model is nested within some time-evolving lateral boundary conditions (atmospheric and sea surface). These boundary conditions are then modified in line with global warming to examine the influence on the (freely-evolving) region of interest. This can be used to study weather extremes or any other scenario of interest. The boundary condition modification in \citeauthor{schar_surrogate_1996} is similar to that of \citeauthor[]{lackmann_hurricane_2015}, but the latter allows the atmospheric conditions in the outer domain to evolve freely after initialisation. On the other hand, \citeauthor[]{meredith_crucial_2015} didn't modify the atmospheric boundary conditions, but instead nudged them towards the scenario of interest. The pseudo-global warming approach was used in a study examining the impacts of Typhoon Haiyan by \citet{takayabu_climate_2015}. They used a chain of successively more granular models, starting from a 60 km global ensemble forecast model, and ending at a 740 m ocean wave and storm surge model. Their use of an operational ensemble forecast system is interesting, though they did not focus on the probabilistic aspect of the system, instead using it to develop a plausible worst-case scenario of impacts from the typhoon. More recently, the related `hindcast attribution method' has been developed, often to study convective dynamical events \citep{wehner_estimating_2019}. This method represents a specific application of the pseudo-global warming approach in which either a regional model is forced by lateral boundary conditions from reanalysis of an extreme or a variable-resolution global model hindcast is used. The initial and lateral boundary conditions are then modified to be representative of a world without human influence in order to assess the conditional impact on the event in question. This hindcast attribution method was first established by \citet{pall_diagnosing_2017} to examine anthropogenic contributions to an exceptional Colorado rainfall event in 2013. It has since been used extensively to study human influence on hurricanes by Michael F. Wehner and colleagues \citep{patricola_anthropogenic_2018,reed_forecasted_2020,reed_attribution_2022}. At this point, it is worth mentioning a few differences between the pseudo-global warming and hindcast attribution methods and the forecast-based approach set out here. Although all of these studies employ ensemble simulations, and some do make probabilistic statements \citep{pall_diagnosing_2017}, the reliability of the models used is not assessed and therefore it is tricky to determine the robustness of such probabilistic statements \citep{bellprat_towards_2019,weisheimer_reliability_2014}. This is a key limitation since it means that changes in probability cannot necessarily be accurately estimated from the ensembles (since the quantiles of individual ensembles may not represent the true outcome probabilities). In terms of the nesting of models, if a reanalysis-forced regional model is used, then climate change responses and interactions outside the domain are not taken into account, which may be of importance for particular events. If a variable-resolution global model is used, biases present in the coarsest domain will be inherited and may be relevant to the simulation within the study domain. Finally, the prescribed-SST designs used in all the studies mentioned preclude ocean-atmosphere interactions that may also be important in the development of extremes such as hurricanes.

  \paragraph*{The subseasonal to seasonal forecast approach at BOM}

    Certainly the most similar prior work to the forecast-based approach presented here is that of Pandora Hope and colleagues \citep{hope_contributors_2015,hope_what_2016,hope_determining_2019,hope_subseasonal_2022,wang_initialized_2021}. In \citet{hope_contributors_2015}, a new method of extreme weather attribution was presented involving reinitialised coupled seasonal forecasts from the Australian Bureau of Meteorology's POAMA model. Starting from the operational forecast model at the time in 2014, they first altered the CO$_2$ concentrations from the 2014 level of 400 ppm to a level consistent with the conditions in 1960 of 315 ppm. They then estimated the anomaly required to remove the ocean's response to these two distinct atmospheric concentration levels by integrating two sets of free-running coupled simulations for 30 years. Two `recent' climate simulations were intialised from observed initial conditions in 2000 and 2010, and two `1960s' climate simulations from 1960 and 1970. An average over the last five years of the sets was used to create temperature and salinity perturbations consistent with the modelled ocean's response to the different CO$_2$ levels. They found that the probability of the record warm Australian spring of 2014 was significantly reduced in these 1960s condition forecasts. \citet{hope_what_2016} further extended this methodology by also modifying the initial land-surface and atmosphere conditions of the model. The land-surface (soil moisture and temperature) and atmosphere (humidity and temperature) perturbations are determined in a manner similar to the ocean, but with much shorter integrations of 2 months over each of the 15 years spanning 2000-2014 and using either true observed initial conditions or the modified CO$_2$ and ocean conditions as in \citet{hope_contributors_2015}. This complete perturbed CO$_2$-ocean-atmosphere-land seasonal forecast attribution system is described by \citet{wang_initialized_2021}. It was used by \citet{hope_what_2016} to assess another record-breaking heat event, and by \citet{hope_determining_2019} to examine anthropogenic influence on the precursors of fire-weather. It now forms the basis for a near real-time `Event Explainer' service, as described by \citet{hope_subseasonal_2022}.

    There is evidently considerable overlap between the BOM approach described above and in \citet{hope_subseasonal_2022} (henceforth H22) and the methodology developed in this thesis throughout the course of chapters \ref{ch3} and \ref{ch4} (henceforth L22). Both make use of weather forecast models to analyse extreme events within the limit at which they are predictable within those models. Both perturb the initial and the boundary conditions of the weather models to assess the impact of human influence on such extremes. However, there are a number of clear differences. H22 has previously focussed on subseasonal to seasonal forecasts and timescales, while L22 has concentrated on shorter medium-range timescales. This different temporal focus impacts the types of event that can be analysed by each approach --- with H22 able to examine events taking place over periods longer than the integration length of medium-range forecasts, and L22 able to study sharper and more short-lived events that may not be predictable on seasonal timescales. For example, the H22 approach is suited to analysing monthly mean temperatures \citep{hope_contributors_2015}; while L22 more suited to analysing heatwave peak temperatures as in \hyperref[ch4]{chapter 4}. The longer timescales involved in seasonal forecasts also mean that model drifts may be important, something that I found in \hyperref[ch4]{chapter 4}. The atmospheric model used in the POAMA seasonal forecast system in H22 is coarse, at a horizontal resolution of 250 km. This coarse resolution means that this model will share the difficulties faced by many coupled climate models when it comes to simulating extreme events. An upgraded system is under development at a resolution of 60 km, but this is still insufficient for simulating some key classes of extreme weather such as hurricanes \citep{hope_subseasonal_2022,patricola_anthropogenic_2018}. On the other hand, the operational ensemble prediction system at ECMWF used in L22 has a resolution of approximately 18 km. In terms of the `completeness' of the estimated human influence, H22 is ahead of L22. Thus far, L22 perturbs both the initial ocean and sea-ice conditions, and the CO$_2$ concentration boundary conditions. This is analogous to the perturbations made by \citet[][though they did not perturb the sea ice]{hope_contributors_2015}. However, in the most recent studies, H22 also perturb the land-surface and atmosphere, thus allowing for a more complete estimate of human influence on the extreme event of interest. In particular, leaving the atmosphere unperturbed requires it to adjust over the course of the forecast, which is a clear limitation on the interpretation of results from this approach. I discuss this limitation and potential methods to resolve it below. It would be very desirable for the two approaches to reach a point at which they were consistent enough to start providing multi-model attribution results, in order to further increase the robustness of forecast-based statements.

  \paragraph*{Spanning uncertainty in climate projections of extreme weather}

    Although the focus of this section is looking at work related to the forecast-based approach to attribution that I have developed, here I also briefly discuss some prior studies done on projections of climate extremes relevant to \hyperref[ch5]{chapter 5}, particularly on how to capture the associated range of uncertainty. I have split these into two broad groups for clarity: dynamical and statistical modelling studies.

    The primary challenge posed by climate extremes is that they are, by their nature, rare, and therefore typically require a considerable amount of data to make inferences about. One increasingly common approach to addressing this requirement is to use large ensembles of climate model simulations. These may be multi-model ensembles, such as the CMIP ensembles \citep{taylor_overview_2012,eyring_overview_2016}; single model initial condition large ensembles, which are used to separate internal climate variability from forced responses \citep{maher_large_2021}; PPEs, such as the global UKCP18 ensemble used in \hyperref[ch5]{chapter 5} \citep{lowe_ukcp18_2018}; or combinations of the above that aim to span as complete an uncertainty space as possible \citep{stainforth_uncertainty_2005,frame_climatepredictionnet_2009}. All of these types of large ensemble explore slightly different components of climate projection uncertainty, and can clearly answer a range of useful questions about future climate extremes. However, none are designed specifically for the study of extremes in the same way that the ExSamples methodology described in \hyperref[ch5]{chapter 5} is. The ExSamples method also explores a different component of uncertainty to each of these large ensemble approaches: atmospheric internal variability (as opposed to combined atmosphere-ocean internal variability). The most similar approach to ExSamples, unsurprisingly, is that which it was based on: the very large atmosphere-only simulations performed by \emph{climateprediction.net} for attribution of extreme weather \citep{pall_anthropogenic_2011,schaller_human_2016}. However, unlike ExSamples, all the previous work using this approach simulated the present and the past, rather than the future. One interesting recent approach developed explicitly for studying and simulating the most extreme events possible is `ensemble boosting'. As described by \citet{gessner_very_2021}, ensemble boosting locates the most extreme events in pre-existing climate projections, and then reinitialises an initial condition ensemble a few days before, in order to see if it could have been even more extreme. Ensemble boosting could therefore be very useful for delving into what the maximum possible extremes might be in the future --- but is not designed to provide the same richness in the variety of future extremes that ExSamples explores.

    The main alternative to such an extreme-specific methodology as ExSamples is to apply statistical models to existing climate model projections (such as the large ensembles described above). One such alternative would be the extreme value-based approach of \citet{brown_climate_2014}. They use an extreme value distribution that depends on the global mean temperature and accounts for model bias to predict changes in future regional extremes. This statistical approach can derive useful additional information from existing model simulations at low computational cost. However, it cannot provide the same multivariate and physically coherent extreme samples as ExSamples. In addition, my results in \hyperref[ch5]{chapter 5} suggested that extreme value theory may underestimate the likelihood of the most extreme events. A different possible methodology for exploring the uncertainties in extremes could be to use a weather generator trained on existing model projections \citep{yiou_anawege_2014}. \citet{yiou_simulation_2020} used a circulation analogue-based approach to estimate how hot European summers could get in the present-day. This approach does provide some physical coherence by linking the estimated temperatures directly to the atmospheric circulation. However, it does not include any other feedbacks, such as soil moisture, would be complex to obtain multivariate information from, and cannot produce extremes driven by processes (or combinations of processes) that lie outside of the training data. As such, it is more suited to providing information about long-term extremes (ie. on seasonal timescales) than about short-term extremes such as the Pacific Northwest heatwave. The lack of feedbacks may mean that the likelihood and intensity of the most extreme events is underestimated; though perhaps information about these feedbacks could be included in the future \citep{suarez-gutierrez_dynamical_2020}. 

\section{Limitations}\label{discussion:limitations}

  Although I have discussed various limitations within the individual chapters that make up this thesis, in this section I consider some of the limitations of the forecast-based approach to attribution developed and explored here as a whole.

  \paragraph*{Forecast adjustment}

    One key aspect of the counterfactual forecasts performed here is that the model --- or more specifically the model atmosphere and land surface --- adjusts continually to the imposed perturbations throughout the integration. This means that the further into the forecast the event of interest happens, the stronger the attributed impact of those perturbations is. At the same time, as the forecast evolves, this effect becomes more uncertain in general (though not necessarily always), due to the increasing dynamical noise arising from the chaotic nature of the weather system. The combination of increasing strength and uncertainty can make analysing and interpreting the results of the counterfactual forecast experiments difficult. In \hyperref[ch4]{chapter 4} I accounted for this difficulty by making use of the fact that the attributable regional impacts of climate change were near-linearly related to the coincidental measured level of global warming. This linear relationship allowed me to benchmark the estimated impact at each forecast lead time to the same level of global warming, regardless of how adjusted they were at the time of the event in question. However, this linear relationship is not guaranteed for every extreme event, and therefore it would be valuable to find methodologies by which this adjustment could either be reduced or removed entirely. I consider a few ideas to achieve this below.

  \paragraph*{Additional uncertainty dimensions}

    In the experiments performed here, I have only considered uncertainty arising from the chaotic nature of the weather system. However, there are additional uncertainties associated with the approach I have developed. One dimension that has been explored in prior work is the uncertainty in the estimation of the ``human fingerprint'' that is removed from the model initial conditions. For example, \citet{pall_anthropogenic_2011}, who removed such a fingerprint from the SSTs (and SICs) of the inital conditions in their naturalised simulations, used four estimates of the warming pattern based on different coupled climate models. This allowed them to test the sensitivity of their attribution statements to the warming pattern used. This approach of using an ensemble of coupled climate models to derive a corresponding ensemble of human fingerprints in order to more completely sample this dimension of uncertainty space has since been used in several other studies \citep{schaller_human_2016}; though many attribution studies and systems still use a single estimate, often based on a multi-model mean pattern \citep{ciavarella_upgrade_2018,stone_benchmark_2021}. 

    In this thesis, I used a single pattern estimated from observations. I used observations to avoid over-reliance on a single coupled climate model, given the biases and known issues present in such models. Although it would have been extremely interesting to more completely explore this dimension of uncertainty (given observations of the ocean subsurface are by no means perfect, especially pre-2000), limits to computer resources prevented me from doing so within the scope of this thesis. However, I hope that such an exploration could be carried out in the future. Doing so would be conceptually straightforward, simply involving treating historical output from a set of different coupled climate models exactly as if they were the observations that I used. Quantitative attribution results derived from each coupled model estimate could be compared to test the sensitivity of such results to the estimate of the human fingerprint used. Because of the variation in the representation of transient historical climate within coupled climate models, each model-derived fingerprint might have to be scaled by (for example) the present-day level of global warming within the model for consistency \citep{tokarska_past_2020}.

  \paragraph*{Single model}

    Within the core research of this thesis concerning forecast-based attribution, I have used a single model, ECMWF's IFS. There were a number of reasons for this limitation: ECMWF provided a mechanism for me to access the computing resources I required through their special project; there were also individuals at ECMWF who provided the technical expertise I needed to design and perform the counterfactual forecast experiments; the IFS is one of the (if not the) best performing numerical weather prediction models on a global basis \citep{hagedorn_comparing_2012}; and applying the counterfactual forecast methodology to other models would have presented considerable technical challenges that lie beyond the scope of this thesis. However, the quantitative results presented here may be sensitive to this choice of model. 
    
    There is considerable variation in both the global and regional response to external forcing among climate models \cite{meehl_context_2020,seneviratne_regional_2020,masson-delmotte_human_2021,masson-delmotte_earths_2021,masson-delmotte_linking_2021,masson-delmotte_weather_2021}. This variation is the reason why \citet{philip_protocol_2020} suggest that having as large a set of different climate models as possible is important for a probabilistic attribution study. Although still a point that should not be overlooked, I argue that such a multi-model assessment is not as important when using the counterfactual forecast approach introduced here. Firstly, a successful prediction ensures (when combined with a limited validation that the prediction did not occur for the wrong reasons) that the model used is able to represent the physical processes of the event in question, and that vital processes are not missing, as may be the case in some climate models. This grounding in the specific physics of the event means that the simulated response to external forcing is considerably more certain and less model dependent. Secondly, the use of a \emph{reliable} forecast ensemble to assess probability ensures that these probabilities are representative of the full space of possible states of the climate system given the initial conditions of the forecast \citep{murphy_new_1973}. This is not the case for climate model simulations, including PPEs. However, despite these mitigating factors, exploring the sensitivity of the counterfactual forecast approach to the model used is an important question for future research. This could be done by carrying out an identical experiment in (for example) the UKMO's numerical weather prediction systems \citep{walters_met_2017,maclachlan_global_2015}.

  \paragraph*{Single event class}

    This thesis has concentrated on extreme heat events. However, given the major contribution of the thesis to attribution literature has been the forecast-based approach taken, rather than understanding the specific type of extreme event studied, this does represent a limitation. There are good reasons for this focus on heatwaves, given the possible scope of a thesis: they have severe associated impacts; are generally well understood; and have been the subject of a large body of prior attribution literature. However, demonstrating that the forecast-based approach can be used for other classes of extreme event will be vital for the method to be taken up widely. A possible candidate for the next class to study would be a high precipitation event. Attribution of high precipitation extremes is generally more challenging than of heatwaves, due to the smaller spatial scales involved, though there still exists a considerable amount of prior work that addresses this question. The high resolution of weather forecast models certainly makes them a more appropriate tool than coarse climate models for studying localised extremes.

  \paragraph*{Considering additional forcing agents}

    In \hyperref[ch4]{chapter 4}, the `complete' estimate of human influence on the Pacific Northwest Heatwave was derived by removing human influence on ocean heat content (by reducing the 3D ocean temperature) and reducing the levels of CO$_2$ in the atmosphere back to their pre-industrial levels. Although we argue that this represents a good estimate of the total sum of human influence, there are a number of additional sources of anthropogenic forcing on the climate system that may need to be considered in future work. Increased levels of other greenhouse gases such as methane or nitrous oxide have a similar radiative effect to CO$_2$, though the forcing from these other agents is relatively small in magnitude compared to CO$_2$ \citep{masson-delmotte_earths_2021}. The other significant human contribution arises from aerosol emissions. Unlike greenhouse gases, historical aerosol emissions have reduced the energy imbalance of the earth, thus masking some of the global warming caused by greenhouse gases. Additionally, while greenhouse gases are well-mixed throughout the atmosphere, aerosols are highly localised in space due to their short lifetime. This means that their effect on local climate can vary considerably from region to region. In this thesis we only considered forcing from increases in CO$_2$ concentrations since i) forcings from these other sources approximately cancel each other out on global scales \citep{jenkins_quantifying_2021}, and ii) the IFS does not include an interactive atmospheric chemistry model, but instead uses an aerosol climatology \citep{bozzo_aerosol_2020}. There has been relatively little research into the effects of aerosols on heatwaves specifically \citep{horton_review_2016}, but it has been found that aerosol reductions in the future exacerbate increases in heatwave magnitude arising from continued greenhouse gas emissions \citep{zhao_strong_2019}. Including the effect of these additional forcing agents on specific extreme weather events would be an extremely interesting direction for future research. The effect of aerosols may be especially interesting for precipitation extremes, since aerosols are known to have direct impacts on cloud formation. However, while including the radiative effects from other greenhouse gases would be straightforward, and could be done exactly as has been for CO$_2$, including the effects from aerosol emissions would likely be considerably more technically complicated and subject to large uncertainty, though might be possible using a version of IFS that includes a tropospheric aerosol scheme \citep{remy_description_2019}.

\section{Future research directions}\label{discussion:future}

  \subsection{Addressing the rapid atmospheric adjustment}

    In this section, I discuss possibilities for how the methodology used here could be altered in order to remove the issues associated with the rapid adjustment of the forecast model, in the atmosphere and at the land surface, to the perturbed intial state. Removing (or alleviating) this adjustment would considerably simplify the interpretation and analysis of couterfactual forecast experiments.

    \paragraph*{Perturbing the initial atmospheric state}

      The simplest way in which to initialise the model from an atmospheric (and land-surface) state that is closer to thermodynamic equilibrium would be to attempt to perturb it such that it is consistent with the changes made to the oceanic and boundary (CO$_2$) conditions. This approach is based on the pseudo-global warming framework \citep{schar_surrogate_1996}, and has been used by a number of recent attribution studies \citep[][]{pall_diagnosing_2017,patricola_anthropogenic_2018,wehner_estimating_2019,reed_forecasted_2020,reed_attribution_2022}. However, unlike these studies and the original framework, which perturb the lateral boundary and initial conditions of a high-resolution nested regional model, in our case we would need to perturb the forecast model globally. These studies typically perturb 3D thermodynamic fields such as temperature, humidity and geopotential based on simulated climate change in GCMs. However, it would be possible to avoid reliance on such models by estimating anthropogenic fingerprints in these fields from long-term reanalysis data \cite{hersbach_era5_2020,laloyaux_cera-20c_2018} exactly as was done to estimate the ocean state perturbations in \hyperref[ch4]{chapter 4}. The disadvantage of this approach would be that the imposed atmospheric perturbations could cause unexpected changes to the physical processes driving the event in question that would be difficult to distinguish from real attributable changes to the event. For example, changes to the temperature and moisture fields could affect local atmospheric circulation in a way that is not necessarily physically consistent with how the same event might have evolved in a climate without human influence.

    \paragraph*{Assimilating the perturbations}

      The simple perturbation approach could be extended to counter some of the issues with physical consistency by coupling it to the data assimilation procedure used to generate the model initial conditions. Data assimilation aims to create the best possible forecast initial state by combining recent model predictions with observations \citep{kalman_new_1960}. It may therefore have the potential to generate a balanced --- but also physically consistent --- initial state for the couterfactual forecasts. Data assimilation has been previously proposed as a technique that could be used for extreme event attribution by \citet{hannart_dada_2016}, though they suggested using likelihoods output from the data assimilation procedure directly, rather than using the procedure to create initial conditions for couterfactual forecasts. The basic idea would be to replace the operational version of the forecast model with an `unforced' version during the data assimilation cycle. This unforced version would essentially be identical to the perturbed initial and boundary condition model run to produce the counterfactual forecasts in \hyperref[ch4]{chapter 4}. The aim behind this unforced data assimilation cycle would be to produce a physically consistent initial state in the unforced model that is as close as possible (in such a climate without human influence) to the observed state of the climate system. How similar the original operational and unforced counterfactual initial states produced would be would depend heavily on the real-world state at the time of the data assimilation. Although this is a promising idea in theory, it would likely face significant technical challenges. For example, some of the observations used in the data assimilation may also have to be perturbed for the procedure to succeed (especially when using a perturbed ocean state if observations of the ocean are used). Given the incomplete nature of the observations and variety of their sources, altering observations before the data assimilation step could be extremely difficult. These potential challenges mean that collaboration with an expert in the operational data assimilation system used would be essential.

    \paragraph*{An operational approach using successive forecasts}

      The previous two methods could be used to perform a counterfactual forecast for a single event. My final suggestion is potentially simpler than both, but would only work in the case of an operational, regularly run counterfactual forecast system for attribution and projection. This idea would be to use the previous forecast to calculate the perturbation required to create a balanced initial state, and is probably clearest when expressed mathematically. If we write the operational (assimilated) initial state at time $\tau$ as $\chi(\tau)$, the operational `forecast operator', that transforms an initial state into a prediction at time $t$ after initialisation as $G_t^1$, and the equivalent counterfactual operator as $G_t^0$, then operational and counterfactual forecast states $X$ at time $t$ after initialisation time $\tau$ can be written respectively as

      \begin{align*}
        X^0(t|\tau) &= G_t^0[\chi(\tau)] \ \textnormal{and}\\
        X^1(t|\tau) &= G_t^1[\chi(\tau)]\,.
      \end{align*}

      \noindent And the difference between the factual operational state and counterfactual state, as estimated by the physics of the forecast model can be written

      \begin{equation}
        \Delta X(t|\tau) = X^1(t|\tau) - X^0(t|\tau)\,.
      \end{equation}

      \noindent Now for a sufficiently small time $t$, such that the model has significant skill and dynamical noise is low, this $\Delta X$ represents a good, and physically consistent, estimate of the difference between the factual and counterfactual worlds at time $\tau + t$. Hence if we then want to issue a successive forecast at time $\tau + t$, rather than simply using $\chi(\tau+t)$ to initialise both forecasts, we could use $\chi(\tau+t)$ for the operational forecast, and $\chi(\tau+t) - \Delta X(t|\tau)$ for the counterfactual forecast. As this routine is applied to several successive operational and counterfactual forecasts in a row, the differences between $\Delta X(t|\tau)$ and $\Delta X(t|\tau+t)$ should tend to a small (though non zero) value, as $\Delta X$ tends towards the `real' difference between a balanced factual initial state and an analogue state in the counterfactual world. This difference between successive $\Delta X$s won't ever reach zero, since the difference between factual and counterfactual worlds at a certain time is dependent on the climate state at that same time. After performing this routine several times, this $\Delta X$ should tend towards the difference between assimilated factual and counterfactual initial states (ie. as would be obtained by assimilating the perturbations, described above).

      The need to continually apply this adjustment to successive forecasts, perhaps a few days apart, is why this routine would only work in the case of an operational system. Its relative simplicity in comparison to the perturbed data assimilation approach, and physical consistency compared with the simple perturbation approach make it attractive. Conceptually, it is somewhat similar to the methodology developed by \citet{wang_initialized_2021}. They used separate long-running integrations of the same forecast model at different CO$_2$ levels to derive the perturbations applied to the initial conditions for their counterfactual simulations. The main differences are that i) this approach does not require costly separate simulations to determine the perturbations since the perturbations are developed through several successive forecasts, and ii) the perturbations generated here would be more closely linked to the actual state of the climate system at the time the forecasts are initialised. However, despite the advantages of this approach, it is still very likely that there would be technical challenges to address. For example, we would have to ensure that errors in $\Delta X$, possibly arising from dynamical noise or forecast error, do not grow between successive forecasts. If this happened, the factual and counterfactual initial states would move further and further apart, and any differences between the factual and counterfactual forecasts could then not be attributed to human influence because of the confounding error present. The shorter the time between successive runs of such an operational attribution system, the less likely for issues like this to occur. The other clear challenge would be determining what variables to perturb. Although it would be possible to perturb everything in the initial conditions, it might be more robust to only apply this adjustment to thermodynamic variables that have a physical basis for being perturbed (just as in the pseudo-global warming approach). Nevertheless, this approach is an intriguing prospect for robust operational attribution using a weather forecast system. 

  \subsection{Expanding the scope of this work}

    This section explores various tests for the methodology used here that should be carried out in further work to more completely assess the robustness of the approach. These tests could be completed without any major changes to the methodology itself.

    \paragraph*{Alternative extremes}

      As mentioned in the Limitations above, this thesis has focused on heatwaves. However, there are many other weather extremes that are of scientific and public interest. Hence, testing the robustness of the forecast-based approach to other extremes would be a natural next step to take. Given their significant coverage in the literature, a high precipitation event would be a good choice for such a test. I note that there would be potential additional considerations when examining a precipitation extreme compared to a heatwave. Firstly, the smaller spatial scale of such precipitation extremes means that these scales might have to be taken into account during the attribution step --- what if the centre of the extreme shifts in the counterfactual world \citep{schaller_role_2020}? Such shifts mean that, especially if linking the precipitation to flooding impacts, pooled catchments, rather than catchments on an individual basis, may have to be considered. The other significant difference is that precipitation forecasts are typically less skillful than temperature forecasts for lead times of more than a few days \citep{rodwell_medium-range_2006,vitart_evolution_2014,swinbank_tigge_2016,monhart_skill_2018,mishra_multi-model_2019,haiden_evaluation_2021}. This reduced skill (particularly during the 1-2 week period, where temperature forecasts are generally good) means that shorter lead times may have to be used to ensure that the forecast model is still able to capture the extreme event within its ensemble --- even if the forecast is reliable. Reducing the lead time would make addressing the forecast adjustment to the perturbed initial conditions even more important.

    \paragraph*{Alternative forecast models}

      Another limitation discussed above was the use of a single model, ECMWF's IFS. As such, testing the sensitivity of this approach to the particular forecast model used would be an important step. It would make sense to begin with a forecast system that uses the same ocean model as the IFS, NEMO 3.4 in ORCA025Z75 configuration \citep{madec_nemo_2008}. This would allow bit-identical perturbations to be used, thus minimising differences that arise from experimental setup as opposed to those that we are interested in that arise from model choice. However, a shared ocean model is not an absolute necessity as one of the steps taken to produce the ocean state perturbations in \hyperref[ch4]{chapter 4} was to interpolate the perturbations onto the ORCA025Z75 model grid --- in theory any ocean grid (and thus any ocean model) could have been used. The UKMO's medium-range MOGREPS-G and seasonal GloSea5 ensemble forecasting systems both use the NEMO ocean model on the same grid, and so may make good candidates for applying the forecast-based approach in an alternative model \citep{maclachlan_global_2015}.

    \paragraph*{Incorporating perturbation uncertainty}

      Including uncertainties associated with the estimation of the perturbed initial state in the forecast-based approach may be important, as discussed above and shown in previous work \citep{pall_anthropogenic_2011,sparrow_attributing_2018}. The main difficulty of including these uncertainties comes from the additional computational cost: if we were to simply repeat our experiments using climate model-derived estimates of these perturbations the cost would scale with the number of climate models used. In order to get a reasonable representation of the uncertainty, O(10) model estimates would be required \citep[as in][]{sparrow_attributing_2018}. This would immediately increase the computational cost by a factor of 10 --- possibly feasible for a single experiment, but much less desirable from an operational perspective. However, given the current operational ensemble prediction system at ECMWF already uses a 5-member ensemble of ocean analyses from which the 51 forecast ensemble members are initialised from, it is possible that we could use a similar ensemble of perturbations within a single forecast ensemble. There are a number of additional outstanding questions: i) how to choose the climate models from which these perturbations are estimated; ii) whether to give the climate models equal weight, or weight them based on some form of model evaluation \citep[this is especially relevant for the latest generation of climate models, the CMIP6 ensemble,][]{eyring_overview_2016,hausfather_climate_2022}; and iii) if using both observation- and climate model-based estimates, how to combine them.

  \subsection{Alternative applications}

    This thesis has largely focussed on the physical attribution of extreme weather events --- which is an intrinsically backward-looking question. However, the approach explored here has potential to inform future projections of climate change as well. In this section, I discuss forward-looking applications of this work and also how it might further research into attribution and projection of the societal impacts arising from extreme weather, which is a rapidly developing field at the moment.

    \paragraph*{Projections of future extremes}

      The question that this thesis has been concerned with answering is how human influence on the climate over the past century or so has affected the probability and severity of extreme events occuring in the present-day. This question is of considerable importance for the numerous reasons detailed in chapter \ref{intro}. However, an arguably more policy-relevant question is that of how extreme weather events may change in the future --- this is especially critical for adaptation planning \citep{harrington_integrating_2022}. Providing information that is specific enough to be useful in a policy context often requires more granularity than coarse climate models are able to provide. Hence, statistical or dynamical downscaling is typically used in order to increase the utility of climate model simulations --- for example in the UKCP reports \citep{lowe_uk_2009,murphy_uk_2009,lowe_ukcp18_2018,murphy_ukcp18_2018}. However, given the structural errors in current climate models, this approach cannot be expected to provide entirely robust and reliable probabilstic imformation, especially on the scales that adaptation planners require. This was the reason why in 2015 \citeauthor{hazeleger_tales_2015} suggested that a complementary approach would be to construct `what if' scenarios using high-resolution weather forecast models that would be able to provide the specific information required; especially given their position in current extreme weather hazard warning systems \citep{schaller_role_2020}. This approach was called `Tales of future weather' \citep{hazeleger_tales_2015}.

      I suggest that the forecast-based approach developed in this thesis could be an attractive methodology for constructing such Tales. The idea would be to take a set of damaging historical extreme weather events (that were successfully forecast), perturb the forecast initial conditions exactly as done in \hyperref[ch4]{chapter 4}, and thus produce realisations of these events in a warmer world. These future forecasts could then be used to examine how future impacts might be worse than in the present, and thus how adaptation policies could be implemented in order to mitigate such impacts. I have already essentially done this future forecast experiment --- though here they were used to test the linearity of the response, rather than for climate projection specifically. The methodology I have used to calculate the anthropogenic fingerprints to be removed from the forecast initial conditions could not just determine the estimated perturbation between pre-industrial and present-day climates, but between climates separated by specified levels of global warming \citep{hasselmann_optimal_1993}. For instance, one could construct forecast-based Tales for policy-relevant future warming levels of 1.5, 2, 3 and 4 C. One advantage that this forecast-based approach has over a storyline approach \citep[eg.][]{benitez_july_2022} in this context is that ensemble forecasts do not just reproduce the event as it unfolded, but also possible alternative realisations that may be even more extreme. This `ensemble-boosting' aspect of such a forecast-based approach to extreme weather projections could help to ensure potential impacts are not underestimated as a result of limiting our view to the outcome that did occur by exploring the range of physically consistent possible outcomes \citep{gessner_very_2021}.
      
      One issue with using an optimal fingerprinting approach to estimating the perturbations required is that it assumes that the pattern of global warming remains constant into the future, which is not certain to be the case, in particular for the higher levels of warming. The discussion above on incorporating perturbation uncertainty is relevant to this \citep{zhou_greater_2021}. For example, perturbations to particular levels of global warming could be derived from coupled climate models in addition to observations in order to more completely span the space of possible future patterns of warming. Another apparent issue with this forecast-based approach arises due to the reliance on historically damaging events. The length of the historical record means that regional coverage of such events will vary considerably. For example, while many regions may have experienced 1-in-100 to 1-in-1000 year heatwaves over the course of the historical record, many will not have. This could potentially leave these regions under-informed in terms of the risk from climate change exacerbated extremes. However, there are already-developed approaches to counter this issue in the literature. One of the most relevant is the UNSEEN approach \citep{thompson_high_2017,kelder_using_2020}. Briefly, this approach uses seasonal ensemble hindcasts to considerably increase the effective sample size of such events within the historical record. I suggest that one way in which useful Tales could be constructed would be to not only look for damaging events in the historical record, but also within the seasonal and medium-range hindcast ensembles that are available. Such `unseen' events could then be re-forecast within a future climate to explore how they may change under continued global warming. A disadvantage of this approach is that such unseen events were not necessarily successfully forecast (which is one of the key features of the forecast-based approaches explored in this thesis), though model fidelity and reliability could be validated in other ways in this case \citep{kelder_interpreting_2022}.

    \paragraph*{Impact assessment}

      Impact attribution is a rapidly growing field of research \citep{perkins-kirkpatrick_attribution_2022,burger_law_2020}. Linking the attributable physical changes due to climate change to the socioeconomic impacts can be extremely powerful as a communication tool and as a way to drive policy \citep{clarke_inventories_2021}. Some examples of this linkage include economic damages from hurricanes \citep{frame_economic_2020,strauss_economic_2021} and mortality from heatwaves \citep{mitchell_attributing_2016,mitchell_climate_2021,lo_estimating_2022}. Despite its clear importance, impact attribution has only taken off recently, possibly due to the difficulties that arise as a result of the additional uncertainties (and non-linearities) associated with linking physical to socioeconomic impacts. This section will not be a lengthy discussion, but I suggest that there are a number of reasons why forecast-based approaches could complement and advance current approaches.

      One key reason, that I have mentioned previously, is that weather forecasts are already built into the modelling chains used to assess risk from extreme weather by combining physical hazard and vulnerability information \citep{schaller_role_2020}. Given how important the vulnerability aspect of extreme weather risk is \citep{raju_stop_2022,mitchell_increased_2022}, using models already familiar to those with relevant expertise is a significant advantage. In additional to this familiarity advantage, the fact that weather forecasts are already key components of many well-validated impact prediction systems \citep[for example, the GloFAS flood warning system][]{alfieri_glofas_2013} means that impact attribution may be able to be carried out with very little technical work --- simply by switching operational weather forecasts for counterfactual ones. For example, \citet{wilkinson_consequence_2022} developed a scheme for translating weather forecasts into damages. Such a scheme could essentially be used `as is' to generate estimates of attributable damages to climate change. Trustworthy estimates of attributable damages could further support litigation relating to increased extreme weather risk. The arguments that I have made at length for forecast-based approaches in this thesis are also very relevant here: significantly increased resolution, well-established reliability, implicit and explicit model validation and event specificity. This is certainly not to say that climate models cannot provide useful information about general impacts arising from climate change (for example, through the storyline framework), but for assessing risks from many types of extreme weather, I argue that weather forecast models would be a more robust tool \citep{palmer_simple_2018}.

\section{Concluding remarks}\label{discussion:remarks}

  In this thesis, I have explored a number of way in which to perform attribution and projection of extreme weather. I have focused on developing a forecast-based approach to attribution based on using reliable operational models that were unequivocally able to simulate the event of interest, as demonstrated by a successful prediction. This approach not only increases the confidence we can have in attribution statements made, but also ensures that we are answering the specific question of how human influence on the climate has affected the individual event in question. A final key benefit of this approach is that it is based on models that are already run operationally, thus potentially opening the door to an operational attribution service that could mitigate the existing selection bias in extreme weather attribution studies. I additionally investigated a novel methodology for producing a rich set of samples of future extreme weather using an atmosphere-only model. This work could produce information relevant to the limits of adaptation in the future, based on the wide variety of extreme scenarios it generates. I argue this methodology would be complemented by a forecast-based approach to climate projection of extremes that could provide a more specific and detailed understanding of the most damaging events. 

  A key scientific limitation of the forecast-based approach I have developed lies in the initialisation of the model. As performed in this thesis, I have not perturbed the initial atmospheric or land-surface state. How to do this robustly is an important question for future work: though I have suggested a few ways in which this might be done. In order to increase the impact of this approach, I suggest that other directions for future work would be to look into different classes of extreme besides the heat events that I have concentrated on in this thesis; and to implement it in other forecast models. Finally, I argue that similar forecast-based approaches could be used to look forwards into the future of extreme weather, or to improve the linkage between physical hazards and their socioeconomic impacts, potentially allowing for attribution and projection of such impacts from individual extreme events in the future.

  I am excited to see how synthesising climate science and weather prediction can inform society about and prepare society for the risks from climate change in the coming years.