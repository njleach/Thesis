\begin{savequote}[8cm]
    Quote
      \qauthor{--- author}
\end{savequote}
    
\chapter{\label{discussion}Discussion} 

Chapter description.
% \small\paragraph{Author contributions:} This chapter is based on the the following publication \footnote{with the author contributions as follows.} \par\vspace{1em}
% \formatchref{Surname, I1. I2., Surname, I1. I2.}{year}{Title}{Journal}{vol}{issue}{pages}{DOI}

\minitoc

\clearpage

\section{An overview of this thesis}

  \blindtext

\section{This thesis in the context of previous work}

  References of relevant work. Hurricane forecast-based \citep{reed_attribution_2022,reed_forecasted_2020,patricola_anthropogenic_2018,lackmann_hurricane_2015,takayabu_climate_2015}. Nested forecast modelling \citep{schaller_role_2020,meredith_crucial_2015}. Initialised (sub)seasonal \citep{hope_contributors_2015,hope_what_2016,hope_determining_2019,hope_subseasonal_2022,wang_initialized_2021,tradowsky_toward_2022,stone_effect_2022}. DADA \citep{hannart_dada_2016}. Probabilistic methodology ref \citep{pall_anthropogenic_2011}. Pseudo global warming \citep{schar_surrogate_1996}.

\section{Limitations}

  Although I have discussed various limitations of the individual studies that make up this thesis, in this section I consider some of the limitations of the forecast-based approach to attribution developed and explored here as a whole.

  \paragraph*{Forecast adjustment}

    One key aspect of the counterfactual forecasts performed here is that the model --- or more specifically the model atmosphere and land surface --- adjusts continually to the imposed perturbations throughout the integration. This means that the further into the forecast the event of interest happens, the stronger the attributed impact of those perturbations is. At the same time, as the forecast evolves, this effect becomes more uncertain in general (though not necessarily always), due to the increasing dynamical noise arising from the chaotic nature of the weather system. This combination of this increasing strength and uncertainty can make analysing and interpreting the results of the counterfactual forecast experiments difficult. In chapter \ref{ch4} I alleviated this difficulty by making use of the fact that the attributable regional impacts of climate change were near-linearly related to the coincidental measured level of global warming. This linear relationship allowed me to benchmark the estimated impact at each forecast lead time to the same level of global warming, regardless of how adjusted they were at the time of the event in question. However, this linear relationship is not guaranteed for every extreme event, and therefore it would be valuable to find methodologies by which this adjustment could either be reduced or removed entirely. I consider a few ideas to achieve this below.

  \paragraph*{Additional uncertainty dimensions}

    In the experiments performed here, I have only considered uncertainty arising from the chaotic nature of the weather system. However, there are additional uncertainties associated with the approach I have developed. One dimension that has been explored in prior work is the uncertainty in the estimation of the ``human fingerprint'' that is removed from the model initial conditions. For example, \citet{pall_anthropogenic_2011}, who removed such a fingerprint from the SSTs (and sea ice fields) of the inital conditions in their naturalised simulations, used four estimates of the warming pattern based on different coupled climate models. This allowed them to test the sensitivity of their attribution statements to the warming pattern used. This approach of using an ensemble of coupled climate models to derive a corresponding ensemble of human fingerprints in order to more completely sample this dimension of uncertainty space has since been used in several other studies \citep{schaller_human_2016}; though many attribution studies and systems still use a single estimate, often based on a multi-model mean pattern \citep{ciavarella_upgrade_2018,stone_benchmark_2021}. 

    In this thesis, I used a single pattern estimated from observations. I used observations to avoid over-reliance on a single coupled climate model, given the biases and known issues present in such models. Although it would have been extremely interesting to more completely explore this dimension of uncertainty (given observations of the ocean subsurface are by no means perfect, especially pre-2000), limits to computer resources prevented me from doing so within the scope of this thesis. However, I hope that such an exploration could be carried out in the future. Doing so would be conceptually straightforward, simply involving treating historical output from a set of different coupled climate models exactly as if they were the observations that I used. Quantitative attribution results derived from each coupled model estimate could be compared to test the sensitivity of such results to the estimate of the human fingerprint used. Because of the variation in the representation of transient historical climate within coupled climate models, each model-derived fingerprint might have to be scaled by (for example) the present-day level of global warming within the model for consistency \citep{tokarska_past_2020}.

  \paragraph*{Single model}

    Within the core research of this thesis concerning forecast-based attribution, I have used a single model, ECMWF's IFS. There were a number of reasons for this limitation: ECMWF provided a mechanism for me to access the computing resources I required through their special project; there were also individuals at ECMWF who provided the technical expertise I needed to design and perform the counterfactual forecast experiemnts; the IFS is one of the (if not the) best performing numerical weather prediction models on a global basis \citep{hagedorn_comparing_2012}; and applying the couterfctual forecast methodology to other models would have presented considerable technical challenges that lie beyond the scope of this thesis. However, the quantitative results presented here may be sensitive to this choice of model. 
    
    There is considerable variation in both the global and regional response to external forcing among climate models \cite{meehl_context_2020,seneviratne_regional_2020,masson-delmotte_human_2021,masson-delmotte_earths_2021,masson-delmotte_linking_2021,masson-delmotte_weather_2021}. This variation is the reason why \citet{philip_protocol_2020} suggest that having as large a set of different climate models as possible is important for a probabilistic attribution study. Although still a point that should not be overlooked, I argue that such a multi-model assessment is not as important when using the counterfactual forecast approach introduced here. Firstly, a successful prediction ensures (when combined with a limited validation that the prediction did not occur for the wrong reasons) that the model used is able to represent the physical processes of the event in question, and that vital processes are not missing, as may be the case in some climate models. This grounding in the specific physics of the event means that the simulated response to external forcing is considerably more certain and less model dependent. Secondly, the use of a \emph{reliable} forecast ensemble to assess probability ensures that these probabilities are representative of the full space of possible states of the climate system given the initial conditions of the forecast \citep{murphy_new_1973}. This is not the case for climate model simulations, including perturbed parameter ensembles. However, despite these mitigating factors, exploring the sensitivity of the couterfactual forecast approach to the model used is an important question for future research. This could be done by carrying out an identical experiment in (for example) the Met Office's numerical weather prediction systems \citep{walters_met_2017,maclachlan_global_2015}.

  \paragraph*{Single event class}

    This thesis has concentrated on extreme heat events. However, given the major contribution of the thesis to attribution literature has been the forecast-based approach taken, rather than understanding the specific type of extreme event studied, this does represent a limitation. There are good reasons for this focus on heatwaves, given the possible scope of a thesis: they have severe associated impacts; are generally well understood; and have been the subject of a large body of prior attribution literature. However, demonstrating that the forecast-based approach can be used for other classes of extreme event will be vital for the method to be taken up widely. A possible candidate for the next class to study would be a high precipitation event. Attribution of high precipitation extremes is generally more challenging than of heatwaves, due to the smaller spatial scales involved, though there still exists a considerable amount of prior work that addresses this question. The high resolution of weather forecast models certainly makes them a more appropriate tool than coarse climate models for studying localised extremes.

  \paragraph*{Considering additional forcing agents}

    In chapter \ref{ch4}, the `complete' estimate of human influence on the Pacific Northwest Heatwave was derived by removing human influence on ocean heat content (by reducing the 3D ocean temperature) and reducing the levels of CO$_2$ in the atmosphere back to their pre-industrial levels. Although we argue that this represents a good estimate of the total sum of human influence, there are a number of additional sources of anthropogenic forcing on the climate system that may need to be considered in future work. Increased levels of other greenhouse gases such as methane or nitrous oxide have a similar radiative effect to CO$_2$, though the forcing from these other agents is relatively small in magnitude compared to CO$_2$ \citep{masson-delmotte_earths_2021}. The other significant human contribution arises from aerosol emissions. Unlike greenhouse gases, historical aerosol emissions have reduced the energy imbalance of the earth, thus masking some of the global warming caused by greenhouse gases. Additionally, while greehouse gases are well-mixed throughout the atmosphere, aerosols are highly localised in space due to their short lifetime. This means that their effect on local climate can vary considerably from region to region. In this thesis we only considered forcing from increases in CO$_2$ concentrations since i) forcings from these other sources approximately cancel each other out on global scales, and ii) the IFS does not include an interactive atmospheric chemistry model, but instead uses an aerosol climatology \citep{bozzo_aerosol_2020}. There has been relatively little research into the effects of aerosols on heatwaves specifically \citep{horton_review_2016}, but it has been found that aerosol reductions in the future exacerbate increases in heatwave magnitude arising from continued greenhouse gas emissions \citep{zhao_strong_2019}. Including the effect of these additional forcing agents on specific extreme weather events would be an extremely interesting direction for future research. The effect of aerosols may be especially interesting for precipitation extremes, since aerosols are known to have direct impacts on cloud formation. However, while including the radiative effects from other greenhouse gases would be straightforward, and could be done exactly as has been for CO$_2$, including the effects from aerosol emissions would likely be considerably more technically complicated and subject to large uncertainty, though might be possible using a version of IFS that includes a tropospheric aerosol scheme \citep{remy_description_2019}.

\section{Future research directions}

  \subsection{Addressing the rapid atmospheric adjustment}

    In this section, I discuss possibilities for how the methodology used here could be altered in order to remove the issues associated with the rapid adjustment of the forecast model, in the atmosphere and at the land surface, to the perturbed intial state. Removing (or alleviating) this adjustment would considerably simplify the interpretation and analysis of couterfactual forecast experiments.

    \paragraph*{Perturbing the initial atmospheric state}

      The simplest way in which to initialise the model from an atmospheric (and land-surface) state that is closer to thermodynamic equilibrium would be to attempt to perturb it such that it is consistent with the changes made to the oceanic and boundary (CO$_2$) conditions. This approach is based on the pseudo-global warming framework \citep{schar_surrogate_1996}, and has been used by a number of recent attribution studies \citep[][]{pall_diagnosing_2017,patricola_anthropogenic_2018,wehner_estimating_2019,reed_forecasted_2020,reed_attribution_2022}. However, unlike these studies and the original framework, which perturb the lateral boundary and initial conditions of a high-resolution nested regional model, in our case we would need to perturb the forecast model globally. These studies typically perturb 3D thermodynamic fields such as temperature, humidity and geopotential based on simulated climate change in GCMs. However, it would be possible to avoid reliance on such models by estimating anthropogenic fingerprints in these fields from long-term reanalysis data \cite{hersbach_era5_2020,laloyaux_cera-20c_2018} exactly as was done to estimate the ocean state perturbations in chapter \ref{ch4}. The disadvantage of this approach would be that the imposed atmospheric perturbations could cause unexpected changes to the physical processes driving the event in question that would be difficult to distinguish from real attributable changes to the event. For example, changes to the temperature and moisture fields could affect local atmospheric circulation in a way that is not necessarily physically consistent with how the same event might have evolved in a climate without human influence.

    \paragraph*{Assimilating the perturbations}

      The simple perturbation approach could be extended to counter some of the issues with physical consistency by coupling it to the data assimilation procedure used to generate the model initial conditions. Data assimilation aims to create the best possible forecast initial state by combining recent model predictions with observations \citep{kalman_new_1960}. It may therefore have the potential to generate a balanced --- but also physically consistent --- initial state for the couterfactual forecasts. Data assimilation has been previously proposed as a technique that could be used for extreme event attribution by \citet{hannart_dada_2016}, though they suggested using likelihoods output from the data assimilation procedure directly, rather than using the procedure to create initial conditions for couterfactual forecasts. The basic idea would be to replace the operational version of the forecast model with an `unforced' version during the data assimilation cycle. This unforced version would essentially be identical to the perturbed initial and boundary condition model run to produce the counterfactual forecasts in chapter \ref{ch4}. The aim behind this unforced data assimilation cycle would be to produce a physically consistent initial state in the unforced model that is as close as possible (in such a climate without human influence) to the observed state of the climate system. How similar the original operational and unforced counterfactual initial states produced would be would depend heavily on the real-world state at the time of the data assimilation. Although this is a promising idea in theory, it would likely face significant technical challenges. For example, some of the observations used in the data assimilation may also have to be perturbed for the procedure to succeed (especially when using a perturbed ocean state if observations of the ocean are used). Given the incomplete nature of the observations and variety of their sources, altering observations before the data assimilation step could be extremely difficult. These potential challenges mean that collaboration with an expert in the operational data assimilation system used would be essential.

    \paragraph*{An operational approach using successive forecasts}

      The previous two methods could be used to perform a counterfactual forecast for a single event. My final suggestion is potentially simpler than both, but would only work in the case of an operational, regularly run couterfactual forecast system for attribution and projection. This idea would be to use the previous forecast to calculate the perturbation required to create a balanced initial state, and is probably clearest expressed mathematically. If we call the operational (assimilated) initial state at time $\tau$ be $\chi(\tau)$, the operational `forecast operator', that transforms an initial state into a prediction at time $t$ after initialisation $G_t^1$, and the equivalent counterfactual operator $G_t^0$, then operational and counterfactual forecast states $X$ at time $t$ after initialisation time $\tau$ can be written respectively as

      \begin{align*}
        X^0(t|\tau) &= G_t^0[\chi(\tau)] \ \textnormal{and}\\
        X^1(t|\tau) &= G_t^1[\chi(\tau)]\,.
      \end{align*}

      \noindent And the difference between the factual operational state and counterfactual state, as estimated by the physics of the forecast model can be written

      \begin{equation}
        \Delta X(t|\tau) = X^1(t|\tau) - X^0(t|\tau)\,.
      \end{equation}

      \noindent Now for a sufficiently small time $t$, such that the model has significant skill and dynamical noise is low, this $\Delta X$ represents a good, and physically consistent, estimate of the difference between the factual and counterfactual worlds at time $\tau + t$. Hence if we then want to issue a successive forecast at time $\tau + t$, rather than simply using $\chi(\tau+t)$ to initialise both forecasts, we could use $\chi(\tau+t)$ for the operational forecast, and $\chi(\tau+t) - \Delta X(t|\tau)$ for the counterfactual forecast. As this routine is applied to several successive operational and counterfactual forecasts in a row, the differences between $\Delta X(t|\tau)$ and $\Delta X(t|\tau+t)$ should tend to a small (though non zero) value, as $\Delta X$ tends towards the `real' difference between a balanced factual initial state and an analogue state in the counterfactual world. This difference won't ever hit zero, since the difference between factual and counterfactual worlds at a certain time is dependent on the climate state at that time. After performing this routine several times, this $\Delta X$ should tend towards the difference between assimilated factual and counterfactual initial states (ie. as would be obtained by assimilating the perturbations, described above).

      The need to continually apply this adjustment to successive forecasts, perhaps a few days apart, is why this routine would only work in the case of an operational system. Its relative simplicity in comparison to the perturbed data assimilation approach, and physical consistency compared with the simple perturbation approach make it attractive. Conceptually, it is somewhat similar to the methodology developed by \citet{wang_initialized_2021}. They used separate long-running integrations of the same forecast model at different CO$_2$ levels to derive the perturbations applied to the initial conditions for their counterfactual simulations. The main differences are that i) this approach does not require costly separate simulations to determine the perturbations since the perturbations are developed through several successive forecasts, and ii) the perturbations generated here would be more closely linked to the actual state of the climate system at the time the forecasts are initialised. However, despite the advantages of this approach, it is still very likely that there would be technical challenges to address. For example, we would have to ensure that errors in $\Delta X$, possibly arising from dynamical noise or forecast error, do not grow between successive forecasts. If this happened, the factual and counterfactual initial states would move further and further apart, and any differences between the factual and counterfactual forecasts could then not be attributed to human influence because of the confounding error present. The shorter the time between successive runs of such an operational attribution system, the less likely for issues like this to occur. The other clear challenge would be determining what variables to perturb. Although it would be possible to perturb everything in the initial conditions, it might be more robust to only apply this adjustment to thermodynamic variables that have a physical basis for being perturbed (just as in the pseudo-global warming approach). Nevertheless, this approach is an intriguing prospect for robust operational attribution using a weather forecast system. 

  \subsection{Expanding the scope of this work}

    This section explores various tests for the methodology used here that should be carried out in further work to more completely assess the robustness of the approach. These tests could be completed without any major changes to the methodology itself.

    \paragraph*{Alternative extremes}

      As mentioned in the Limitations above, this thesis has focused on heatwaves. However, there are many other weather extremes that are of scientific and public interest. Hence, testing the robustness of the forecast-based approach to other extremes would be a natural next step to take. Given their significant coverage in the literature, a high precipitation event would be a good choice for such a test. I note that there would be potential additional considerations when examining a precipitation extreme compared to a heatwave. Firstly, the smaller spatial scale of such precipitation extremes means that these scales might have to be taken into account during the attribution step --- what if the center of the extreme shifts in the counterfactual world \citep{schaller_role_2020}? Such shifts mean that, especially if linking the precipitation to flooding impacts, pooled catchments, rather than catchments on an individual basis, may have to be considered. The other significant difference is that precipitation forecasts are typically less skillful than temperature forecasts for lead times of more than a few days \citep{rodwell_medium-range_2006,vitart_evolution_2014,swinbank_tigge_2016,monhart_skill_2018,mishra_multi-model_2019,haiden_evaluation_2021}. This reduced skill (particularly during the 1-2 week period, where tempreature forecasts are generally still good) means that shorter lead times may have to be used to ensure that the forecast model is still able to capture the extreme event within its ensemble --- even if the forecast is reliable. Reducing the lead time would make addressing the forecast adjustment to the perturbed initial conditions even more important.

    \paragraph*{Alternative forecast models}

      Another limitation discussed above was the use of a single model, ECMWF's IFS. As such, testing the sensitivity of this approach to the particular forecast model used would be an important step. It would make sense to begin with a forecast system that uses the same ocean model as the IFS, NEMO 3.4 in ORCA025Z75 configuration \citep{madec_nemo_2008}. This would allow bit-identical perturbations to be used, thus minimising differences that arise from experimental setup as opposed to those that we are interested in that arise from model choice. However, a shared ocean model is not an absolute necessity as one of the steps taken to produce the ocean state perturbations in chapter \ref{ch4} was to interpolate the perturbations onto the ORCA025Z75 model grid --- in theory any ocean grid (and thus any ocean model) could have been used. The Met Office's medium-range MOGREPS-G and seasonal GloSea5 ensemble forecasting systems both use the NEMO ocean model on the same grid, and so may make good candidates for applying the forecast-based approach in an alternative model \citep{maclachlan_global_2015}.

    \paragraph*{Incorporating perturbation uncertainty}

      Including uncertainties associated with the estimation of the perturbed initial state in the forecast-based approach may be important, as discussed above and shown in previous work \citep{pall_anthropogenic_2011,sparrow_attributing_2018}. The main difficulty of including these uncertainties comes from the additional computational cost: if we were to simply repeat our experiments using climate model-derived estimates of these perturbations the cost would scale with the number of climate models used. In order to get a reasonable representation of the uncertainty, O(10) model estimates would be required \citep[as in][]{sparrow_attributing_2018}. This would immediately increase the computational cost by a factor of 10 --- possibly feasible for a single experiment, but much less desirable from an operational perspective. However, given the current operational ensemble prediction system at ECMWF already uses a 5-member ensemble of ocean analyses from which the 51 forecast ensemble members are initialised from, it is possible that we could use a similar ensemble of perturbations within a single forecast ensemble. There are a number of additional outstanding questions: i) how to choose the climate models from which these perturbations are estimated; ii) whether to give the climate models equal weight, or weight them based on some form of model evaluation \citep[this is especially relevant for the latest generation of climate models, the CMIP6 ensemble,][]{eyring_overview_2016,hausfather_climate_2022}; and iii) if using both observation- and climate model-based estimates, how to combine them?

  \subsection{Alternative applications}

    This thesis has largely focussed on the physical attribution of extreme weather events --- which is an intrinsically backward-looking question. However, the approach explored here has potential to inform future projections of climate change as well. In this section, I discuss forward-looking applications of this work and also how it might further research into attribution and projection of the societal impacts arising from extreme weather, which is a rapidly developing field at the moment.

    \paragraph*{Projections of future extremes}

      The question that this thesis has been concerned with answering is how human influence on the climate over the past century or so has affected the probability and severity of extreme events occuring in the present-day. 

    \paragraph*{Impact assessment}

      

\section{Concluding remarks}