\begin{savequote}[8cm]
    Quote
      \qauthor{--- author}
\end{savequote}
    
\chapter{\label{intro}Introduction} 

In this chapter I introduce the problem of attribution of individual extreme weather events to anthropogenic climate change. I review the current methodologies and frameworks that address this problem, in particular the contrasting storyline and probabilistic approaches to attribution. Although these frameworks are gaining acceptance and maturity, I suggest that a weather forecast-based approach could further increase the trustworthiness of attribution studies. Finally, I provide a conceptual sketch of these various attribution frameworks within a simple non-linear dynamical system.
\small\paragraph{Author contributions:} This chapter is based on the the following publication \footnote{with the author contributing as follows.} \par\vspace{1em}
\formatchref{Surname, I1. I2., Surname, I1. I2.}{year}{Title}{Journal}{vol}{issue}{pages}{DOI}

\minitoc

\clearpage

\section{The problem of extreme event attribution}

  % detection and attribution of climate change (hasselmann->gillet)
  % Being specific about the question ("cause" vs influence, hannart)
  % scientific challenges posed by EEA
  Review papers: \citep{allen_scientific_2007,stott_attribution_2013,stott_attribution_2016,otto_attribution_2016,otto_attribution_2017,swain_attributing_2020,easterling_detection_2016,noauthor_attribution_2016}

\section{Motivating the question}

  Now that I have posed the question, before I move on to how we might answer it, I think it would be useful for me to discuss why we want to answer it. In short: \emph{what's the point of this thesis?}

  % start off with liability (allen03)
  In 2003, Myles Allen wrote \citetitle{allen_liability_2003} \citep{allen_liability_2003}. This commentary is widely acknowledged as the first time the idea that individual extreme events could be attributed to external drivers such as human influence was proposed \citep{otto_attribution_2017}. Although \citeauthor{allen_liability_2003} touched on both methodology and motivation for extreme event attribution, here I shall focus on the latter aspect. The motivation behind extreme event attribution as proposed in \citetitle{allen_liability_2003} is compensation for damage to individuals caused by climate change or, as \citeauthor{allen_liability_2003} puts it, 
  \begin{quote}
    Will it ever be possible to sue anyone for damaging the climate?
  \end{quote}
  \citeauthor{allen_liability_2003} suggests that in the future those affected by particular extreme weather may, given sufficient scientific certainty, be able to claim compensation from greehouse gas emitters for damages caused by the extreme weather. He proposes a framework, grounded in concepts from epidemiology \citep{stone_end--end_2005}, in which emitters pay for the `fraction' of an extreme weather event that they caused, even in the abscence of absolute causation. This fraction is estimated probabilistically based on the change in likelihood of the event in a world in which the emissions never happened (ie. if the event is half as likely to occur without the emissions, then the fraction of the event that is attributable to the emitters is 50\%).
  This specific application is therefore using extreme event attribution as evidence in environmental tort law. Since \citet{allen_liability_2003}, much has been written in relation to this application. \citet{allen_scientific_2007} presents an overview of the state of climate change detection and attribution aimed at legal professionals, concluding with a set of related questions for the legal community. More recently, \citet{stuart-smith_filling_2021} provided a set of suggestions for potential plaintiffs on how to best make use of the climate science available (noting that evidence used in previous cases `lags substantially behind the state of the art'). Coming from the other side of the coin, \citet{marjanac_acts_2017} provide suggestions for climate change scientists, emphasising that `clear and confident expression of science in a manner that can be applied by non-scientists, including lawyers' is key. Elisabeth A. Lloyd has authored a number of studies exploring various issues including the different standards of proof in scientific and legal contexts \citep{lloyd_climate_2021}; how different approaches to extreme event attribution can complement one another to provide the most useful picture of climate change impacts for a broad range of contexts \citep{lloyd_climate_2018,lloyd_environmental_2020}; and finally, examines a specific tort law case that made use of extreme event attribution \citep{lloyd_climate_2021-1}. For thorough review of both the science and legal context of extreme event attribution, written from a legal perspective, with reference to specific cases, I recommend \citet{burger_law_2020,marjanac_extreme_2018}.
  
  In addition to the original application in tort law suggested in \citet{allen_liability_2003}, more recently it has been suggested that extreme event attribution could `play a significant roll in quantifying loss and damage' \citep{wehner_operational_2022}. Loss and damage is generally understood as the unavoidable adverse impacts of climate change \citep{mace_loss_2016}, and has become a key piece of international climate change policy since the inclusion of Article 8 of the Paris Agreement. Several recent extreme event attribution based studies may have considerable influence in the future in this space, including \citet{clarke_inventories_2021}, who set out a framework for recording key details of high-impact weather events as a new source of evidence for global stocktakes on loss and damage; and \citet{otto_assigning_2017,lott_quantifying_2021}, who adapt conventional extreme event attribution approaches to estimate the contributions of \emph{specific} emitters to individual extreme weather events (as opposed to the more usual broad `human influence' considered). Perhaps we don't have long to wait before the question posed by \citeauthor{allen_liability_2003} quoted above is answered?

  The other key nonscientific motivating factor for extreme event attribution is the public engagement and interest in the research  \citep{swain_attributing_2020}. The `headline' number in climate science has for a long time been change in global mean temperature \citep{stocker_climate_2013,ipcc_global_2018,masson-delmotte_climate_2021}. While this is clearly a very important number as the primary metric of the impact that humanity is having on the climate, it is not a number that individuals can easy relate to due to the large scales involved and indirect nature of the associated impacts. On the other hand, extreme weather events are phenomena that are actually experienced in real time by individuals --- and regularly reported on by the media. Since extreme weather events can cause severe and direct socioeconomic impacts \citep{fouillet_excess_2006}, increases in their frequency would likely be a considerably more relatable and concerning consequence of climate change than the distant change in global mean quantities. Previous work has shown that extreme event attribution may be an exceptionally useful tool for climate change communication \citep{ettinger_whats_2021}, though can prove unhelpful if results are not clear and comprehensible for a general audience \citep[for example if different attribution studies regarding a single event appear to provide conflicting headline results][]{osaka_natural_2020}. A specific study investigating the experience-perception link of climate disaters in the context of Floridians that had experienced hurricane Irma found that this experience increased both their belief and concern in global warming \citep{bergquist_experiencing_2019}, though a more recent study looking at connections between local weather and climate change awareness in Germany did not find a link \citep{gartner_experiencing_2021}.

  % improve models (high benchmark of test) - decide whether to include
  \citep{sillmann_understanding_2017}

\section{Answering the question}

  At this point, I have discussed the question that this thesis is primarily concerned with, and the reasons why it is of broad importance. Now, I shall describe and explore the ways in which previous studies have approached this question, in particular focusing on the probabilistic \citep[often ``conventional'',][]{stott_human_2004} and storyline \citep{hoerling_anatomy_2013} frameworks. 

  \subsection{Probabilistic attribution}

    Probabilistic attribution seeks, ultimately, to determine the change in probability of an extreme event arising due to some external driver. This was the approach to extreme event attributon proposed by \citet{allen_liability_2003} and first applied by \citet{stott_human_2004} to the 2003 European heatwave. They applied an optimal fingerprinting technique \citep{hasselmann_optimal_1993,hasselmann_multi-pattern_1997} to transient climate model simulations. They used five simulations, one set of four with all forcings included, plus one with natural forcings only; generating scaling factors of the correspondence between the modelled response and observed changes by regressing each set onto observed central European summer temperature. The scaling factors could then be used to determine the 1990s temperature anomalies attributable to all forcings combined and natural forcings alone. A third control run at a fixed, pre-industrial level of forcing was used to estimate internal variability corresponding likelihood functions for these temperature anomalies. They finally used a peak-over-threshold extreme value analysis of the same control run to determine the probability of exceeding the temperature of the pre-2003 hottest summer in worlds with and without climate change by adjusting the mean summer temperature to the estimated 1990s temperature anomalies both with and without anthropogenic forcing. These probabilities (and associated uncertainty) could be used to determine the likelihood function of the change in risk of the heatwave attributable to human influence. This fairly involved statistical approach (in particular, the necessity for the use of a control run to estimate uncertainty due to internal variability) has been largely replaced by the use of much larger single- or multiple- model ensembles.

    The next advance in probabilistic extreme event attribution came with \citet{van_oldenborgh_how_2007}, who developed a methodology for estimating the change in risk of an extreme event using observations alone. \citeauthor{van_oldenborgh_how_2007} took observed timeseries of autumn temperatures measured by the De Bilt weather station, and removed the climate change signal by regression onto low-pass filtered global mean surface temperature (a reasonable proxy for anthropogenic influence on the climate, given the small contributions from natural forcing). Using this detrended series and extreme value analysis, he then computed the return period of the exceptional warm 2006 autumn in Europe, and compared it to the return period estimated using the original series. This method was later extended to use the return period to compute the return period of the extreme for both the present-day and pre-industrial period by shifting the detrended series by the attributable trend computed in the regession \citep[eg.][]{philip_protocol_2020,leach_anthropogenic_2020}. In this way, the change in risk between pre-industrial and present climates can be estimated. It is worth noting that this methodology does not formally attribute and changes in risk to anthropogenic influence, since trends in local climate may be influenced by other factors, and no use is made of a counterfactual world without human influence on the climate (since no observations of such a world exist). This method can also be applied to transient simulations from climate models.

    The final advance that I shall highlight is from \citet{pall_anthropogenic_2011}, and was the first instance where specific fixed forcing (as opposed to transient) factual and counterfactual simulations were used. \citeauthor{pall_anthropogenic_2011} generated very large (2000+ member) atmosphere-only climate model ensembles of autumn 2000. One ensemble was driven using observed sea surface temperatures and sea ice, and corresponding atmospheric conditions (greenhouse gas, aerosol and ozone concentrations) for that time. The other four were driven using atmospheric conditions for the year 1900, and subtracting four estimates of attributable twentieth-century SST warming from the observed sea surface temperatures; the four estimates of attributable warming were derived from four different coupled climate model simulations. River runoff for England and Wales in the factual ensemble was compared to runoff in the four `naturalised' counterfactual ensembles to determine the difference in risk of exceeding the value actually observed in autumn 2000 in the different climates. In this case, the ensembles are sufficiently large that extreme value analysis was not required (the -- SST conditional -- return period could be calculated by simply counting the number of members that exceeded the observed threshold and dividing by the total ensemble size in each case). This methodology is attractive due to the clear and straightforward statistical analysis of the large ensembles (with no reliance on extreme value analysis or optimal fingerprinting techniques). However, it does generally require very large ensembles of the time period when the event took place (ie. autumn 2000 in this case); and is conditional on the prescribed SST pattern (requiring either that anthropogenic influence isn't affecting interannual modes of SST variability, or that the extreme in question is independent of such modes).

    The `standard' approach to probabilistic attribution in the present draws upon each of these previous advances \citep{philip_protocol_2020}. Here I am taking the World Weather Attribution project (WWA) methodology as standard, since they are the most prolific group both in terms of number of events analysed and media coverage of their results \citep{van_oldenborgh_pathways_2021}. Their approach involves:
    \begin{enumerate}
      \item Defining the event. Extreme events are, by the nature of the weather, exceptionally high dimensional and could be described in a practially unlimited number of ways (ie. what variables to use? What spatial scale? What temporal scale?). However, to be able to analyse changes to such events, we must be able to define them quantatively. The WWA attempts to select a metric that most closely corresponds to the impacts of the extreme event in question, taking in account what questions are being asked by the various stakeholders. For example, if the key impact of interest is heatwave-associated mortality, then peak 3-day moving average daily maximum temperatures may be selected due to their close connection to health impacts \citep{dippoliti_impact_2010}. Once a metric has been chosen by which to define the extreme event, they use a `class-based' framing considering all events of a similar magnitude, often by choosing the annual maximum values of the metric. This class-based framing results in a largely unconditional analysis, which I will discuss further below.
      \item Analysing trends in observed data following \citet{van_oldenborgh_how_2007}. This is typically done by fitting an appropriate distribution whose parameters shift or scale with low-pass filtered global mean surface temperature (GMST). The shifting or scaling depends upon the chosen metric and its observed or expected response to climate change. For example, the temperature based heatwave metrics that are the primary concern of this thesis are typically assumed to simply shift with global warming. From this GMST-covarying distribution, the return period is then computed for present-day and pre-industrial values of GMST. From these returns periods the change in risk of the observed extreme can be calculated. As with \citet{van_oldenborgh_how_2007}, these changes in risk are not strictly attributable to human influence on the climate due to the lack of a no-human counterfactual.
      \item Analysing simulations from as many models as possible. Transient model simulations are analysed in an identical way to observations. Fixed forcing simulations are analysed following \citet{pall_anthropogenic_2011}. Only models which are able to closely represent the observed climate are considered -- evaluated on the basis of the trends and distribution parameters implied by the model data.
      \item Synthesising these various strands of evidence. The aim behind using as many lines of evidence as possible, combining statistical and numerical models, is to try and determine the most robust conclusion possible within the context of the associated uncertainties. 
    \end{enumerate}
    This `standard' approach has been refined over the past decade by the WWA team \citep{van_oldenborgh_pathways_2021}, learning through application to a wide range of locations and types of extreme. The widespread recognition and understanding of extreme event attribution by the general public is due, in no small part, to this approach and how rapidly the WWA team have been able to apply it to extreme events in the past few years. Their rapid response has meant that they are able to answer the questions people ask in the aftermath of such events when they are actually asking them -- rather than following a lengthy peer-review process. However, this standard probabilistic approach to attribution is not without issues of its own -- hence this thesis -- which I shall now discuss.

    The first issue arises due to the unconditional use of climate models. By their nature, extreme events are typically produced by exceptional physical processes, or combinations of processes. The type of models used in extreme event attribution are typically coarse (O(100 km)), and may well not be able to physically represent all the processes involved in the production of specific extreme events even if they can accurately represent the average climate \citep{sillmann_understanding_2017,trenberth_attribution_2015}. Such models still have serious known biases relevant to the simulation of extremes, including poor representation of Euro-Atlantic blocking \citep{schiemann_resolution_2017,dorrington_how_2021}, which is a key synoptic driver of heatwaves over the continent. These biases become even more important when considering not only the models' ability to simulate the present climate, but also their response to external forcings such as anthropogenic climate change \citep{palmer_nonlinear_1999,palmer_simple_2018}. The use of biased models can lead to potentially incorrect quantitative attribution statements \citep{bellprat_attribution_2016,bellprat_towards_2019}. 
    
    The second key issue derives from the treatment of individual extremes as one of an event class. For example, in the conventional probabilistic approach to attributing a particular heatwave, one might consider all the previous annual maximum temperatures (eg. in order to apply extreme value analysis as discussed above). However, the particular heatwave in question might have arisen from a very different -- possibly unique in the context of the relatively short historical record -- set of meteorological circumstances and drivers compared to all the previous heatwaves. This not only renders such extreme value analysis as is often performed potentially inappropriate (as the heatwave in question is drawn from a different underlying distribution to the others), but also any estimated climate change responses. What if the combination of the particular processes involved in producing the heatwave in question responds fundamentally differently to the processes that have generated past heatwaves?
    
    The final issue I shall discuss is the one that has been most often commented on in previous work: the risk of type II errors \citep{shepherd_common_2016,trenberth_attribution_2015}. This risk arises because some aspects of the climate system response to external forcing are much more certain and well-understood than others. For example, the thermodynamic aspects of climate change are broadly very well understood and certain: rising greenhouse gas concentrations lead to a thicker troposphere, thus raising surface temperatures and increasing the moisture capacity of the troposphere. On the other hand, the dynamic aspects of climate change are considerably less certain, with models often disagreeing over the direction of changes in atmospheric dynamics \citep{masato_winter_2013}. At this point, I note that this is not an entirely independent issue to the first issue discussed since much of this uncertainty arises due to model biases and imperfections. Since extreme events are often driven by a combinations of both thermodynamic and dynamic processes, the very certain thermodynamic climate change impacts can be masked to some extent by the much less certain dynamic climate change impacts. This uncertainty can lead to falsely asserting that there is no overall impact -- a type II error. \citet{trenberth_attribution_2015} argue that it is better to focus on the aspects of the event that are well understood, for example by conditioning analyses upon the large scale circulation of the event in question, thus removing the potentiall very uncertain dynamic aspects of climate change. This suggestion was extended and discussed at length by \citet{shepherd_common_2016}, and has become the basis for the new kid on the block in extreme weather attribution: the storyline approach.

  \subsection{Attribution through storylines}

    The storyline approach \citep[or `Boulder' approach,][]{otto_attribution_2017} aims to determine the contribution of various causal factors that to the extreme event, and considers how anthropogenic climate change has affected those factors (and thus the extreme) in a deterministic manner. This approach was first applied by \citet{hoerling_anatomy_2013} to the 2011 summer combined Texas heatwave and drought. They used a variety of simulations, including atmosphere-only and coupled climate model runs, and seasonal forecasts. They examined the influence of rainfall deficit in the months preceding the heatwave, SST influence on the drought, and the overall predictability of the extreme at the start of May. As such, their intended goal was much broader than just assessing the anthropogenic contribution to the heatwave, aiming to advance the overall predictability of such events by examining all causes, human and natural. 
    % the aims of storyline approaches
    % Hoerling - hazelger - shepherd - benitez
    \citep{hazeleger_tales_2015,shepherd_common_2016,benitez_july_2022}
    % issues: does not provide estimate of change in risk

  \subsection{Do we need new approaches?}

  \subsection{Operationalising attribution}

\section{Conceptualising different approaches}

  % use the dynamicist's favourite "toy": Lorenz63
  % demonstrate "storyline / DADA", conventional attribution
  % propose forecast-based attribution
  \blindtext

\section{What to expect in this thesis}

  % exploratory conventional attribution study 
  % tangent study about climate change projections + link to forecast based approaches
  % first step to forecast-based attribution - partial attribution
  % "full" forecast-based attribution
  % conclusion discussing current limitations + future directions
  \blindtext