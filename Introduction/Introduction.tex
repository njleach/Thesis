\begin{savequote}[8cm]
    Quote
      \qauthor{--- author}
\end{savequote}
    
\chapter{\label{intro}Introduction} 

In this chapter I introduce the problem of attribution of individual extreme weather events to anthropogenic climate change. I review the current methodologies and frameworks that address this problem, in particular the contrasting storyline and probabilistic approaches to attribution. Although these frameworks are gaining acceptance and maturity, I suggest that a weather forecast-based approach could further increase the trustworthiness of attribution studies. Finally, I provide a conceptual sketch of these various attribution frameworks within a simple non-linear dynamical system.
\small\paragraph{Author contributions:} This chapter is based on the following publication \footnote{with the author contributing as follows.} \par\vspace{1em}
\formatchref{Surname, I1. I2., Surname, I1. I2.}{year}{Title}{Journal}{vol}{issue}{pages}{DOI}

\clearpage

\minitoc

\clearpage

\section{The problem of extreme event attribution}

  The link between greenhouse gas emissions and anthropogenic climate change has been known about for over a century \citep{arrhenius_influence_1896}. In recent decades, significant advances have been made in understanding the human influence (or `fingerprint') in multi-decadal trends in global and regional climate \citep{bindoff_detection_2013,masson-delmotte_human_2021,stott_detection_2021}. A number of statistical advances developed around the millennium \citep{hasselmann_optimal_1993,hasselmann_multi-pattern_1997,allen_checking_1999,allen_estimating_2003,stott_estimating_2003} have allowed so-called `detection and attribution' of human and influence on a very wide range of climate variables in the decades since, including global temperature \citep{hegerl_multi-fingerprint_1997,stott_attribution_2001,gillett_constraining_2021}, regional temperature \citep{gillett_attribution_2008}, global precipitation \citep{bonfils_human_2020}, global soil moisture \citep{gu_attribution_2019}, ocean salinity \citep{stott_detection_2008}, tropospheric thickness \citep{christidis_changes_2015}, and many others \citep{masson-delmotte_human_2021}. However, such large-scale and long-term changes in mean climate are far detached from individual human perception \citep{pahl_perceptions_2014}. On the other hand, individuals are able to relate to extreme weather events, due to the immediate and severe impacts they can have. This interest in such damaging events led scientists in the early 2000s to consider whether it would be possible to detect and attribute human influence on an individual extreme weather event \citep{allen_liability_2003}.

  % Being specific about the question ("cause" vs influence, hannart)
  % Dole vs. rahmstorf
  What do I mean by detection and attribution of human influence in the context of a single extreme event? Possibly the simplest way to pose this question in the aftermath of such an event would be to ask: `was this weather \emph{caused} by climate change?'. However, this framing sets the standard of proof at an exceptionally high bar --- if the event were at all possible without human influence in any sense, regardless of how unlikely it might be, then the answer would be `no'. Despite the prevalence of this framing in both scientific and non-scientific circles \citep{allen_scientific_2007,wuebbles_detection_2017}, and some recent studies that claim to have achieved this standard of proof \citep{imada_july_2019}, it is not a particularly useful or informative question to ask, and may have contributed to the belief that attribution of individual weather events is not possible \citep{solow_extreme_2015}. A more relevant and answerable question might be: `has anthropogenic climate change made this event more likely?'. This was the question posed in 2003 by Myles Allen in the seminal commentary \citetitle{allen_liability_2003} \citep{allen_liability_2003}, widely acknowledged as the first time the idea that individual extreme events could be attributed to external drivers such as human influence was proposed \citep{otto_attribution_2017}. This frequency-based question has been tackled in many studies since \citep{stott_human_2004}. An alternative question could be: `has anthropogenic climate change made this event more intense?'. Although \citet{allen_scientific_2007} suggested that it makes little sense in the context of a nonlinear and chaotic weather system, many studies have now examined this magnitude-based question \citep{dole_was_2011}. 

  These two different framings of the same ultimate question have led to some apparent discrepancies at times \citep{otto_reconciling_2012}. By their nature, extreme events nearly always have a significant contribution from the natural internal variability of the weather. This natural contribution often far exceeds any estimated human contributions to the event magnitude. However, these `small' human-induced changes in magnitude can lead significant changes in event probability. In this way, an event can be accurately described as both `mainly natural in origin' \citep{dole_was_2011} and having `an approximate 80 \% probability that it would not have occurred without climate warming' \citep{rahmstorf_increase_2011}. I, and many others \citep{stott_attribution_2013,stott_attribution_2016,otto_attribution_2016,swain_attributing_2020,easterling_detection_2016,national_academies_of_sciences_engineering_and_medicine_attribution_2016} believe that both of these framings are useful. Although I will develop these ideas further in due course, one context in which understanding changes in both probability and magnitude can be important is adaptation to climate change. Changes in probability are important for societal resilience: \emph{how strong do I need to make a flood barrier given an increase in the number of floods of a certain severity?}; while changes in magnitude are vital for preparing for the potentially very non-linear impacts of extreme weather events: \emph{how high do I need to make a flood barrier to ensure that it doesn't fail given the severity of a 1-in-100 year flood has now increased?}. Of course, the picture is more nuanced than this simplistic view, but my underlying point that understanding both changes in probability and magnitude are important is now widely accepted. Hence, the question that this thesis is largely concerned with answering is: \textbf{`how has human influence on the climate affected both the probability and intensity of specific extreme weather events?'}.

  % Do I discuss problems with extreme event attribution here?

\section{Motivating the question}

  Now that I have posed the question, before I move on to how we might answer it, I think it would be useful for me to discuss why we want to answer it. In short: \emph{what's the point of this thesis?}

  % start off with liability (allen03)
  To begin, I return to \citetitle{allen_liability_2003} \citep{allen_liability_2003}, now focusing on the motivation for extreme event attribution. The motivation behind extreme event attribution as proposed in \citetitle{allen_liability_2003} is compensation for damage to individuals caused by climate change or, as \citeauthor{allen_liability_2003} puts it, 
  \begin{quote}
    Will it ever be possible to sue anyone for damaging the climate?
  \end{quote}
  \citeauthor{allen_liability_2003} suggests that in the future those affected by particular extreme weather may, given sufficient scientific certainty, be able to claim compensation from greenhouse gas emitters for damages caused by the extreme weather. He proposes a framework, grounded in concepts from epidemiology \citep{stone_end--end_2005}, in which emitters pay for the `fraction' of an extreme weather event that they caused, even in the absence of absolute causation. This fraction is estimated probabilistically based on the change in likelihood of the event in a world in which the emissions never happened (i.e. if the event is half as likely to occur without the emissions, then the fraction of the event that is attributable to the emitters is 50\%).
  This specific application is therefore using extreme event attribution as evidence in environmental tort law. Since \citet{allen_liability_2003}, much has been written in relation to this application. \citet{allen_scientific_2007} presents an overview of the state of climate change detection and attribution aimed at legal professionals, concluding with a set of related questions for the legal community. More recently, \citet{stuart-smith_filling_2021} provided a set of suggestions for potential plaintiffs on how to best make use of the climate science available (noting that evidence used in previous cases `lags substantially behind the state of the art'). Coming from the other side of the coin, \citet{marjanac_acts_2017} provide suggestions for climate change scientists, emphasising that `clear and confident expression of science in a manner that can be applied by non-scientists, including lawyers' is key. Elisabeth A. Lloyd has authored a number of studies exploring various issues including the different standards of proof in scientific and legal contexts \citep{lloyd_climate_2021}; how different approaches to extreme event attribution can complement one another to provide the most useful picture of climate change impacts for a broad range of contexts \citep{lloyd_climate_2018,lloyd_environmental_2020}; and finally, examines a specific tort law case that made use of extreme event attribution \citep{lloyd_climate_2021-1}. For thorough review of both the science and legal context of extreme event attribution, written from a legal perspective, with reference to specific cases, I recommend \citet{burger_law_2020,marjanac_extreme_2018}.
  
  In addition to the original application in tort law suggested in \citet{allen_liability_2003}, more recently it has been suggested that extreme event attribution could `play a significant role in quantifying loss and damage' \citep{wehner_operational_2022}. Loss and damage is generally understood as the unavoidable adverse impacts of climate change \citep{mace_loss_2016}, and has become a key piece of international climate change policy since the inclusion of Article 8 of the Paris Agreement. Several recent extreme event attribution based studies may have considerable influence in the future in this space, including \citet{clarke_inventories_2021}, who set out a framework for recording key details of high-impact weather events as a new source of evidence for global stocktakes on loss and damage; and \citet{otto_assigning_2017,lott_quantifying_2021}, who adapt conventional extreme event attribution approaches to estimate the contributions of \emph{specific} emitters to individual extreme weather events (as opposed to the more usual broad `human influence' considered). Perhaps we won't have to wait much longer before the question posed by \citeauthor{allen_liability_2003} is answered?

  % should i frame this in terms of the episodic vs semantic memory literature?
  The other key non-scientific motivating factor for extreme event attribution is the public engagement and interest in the research  \citep{swain_attributing_2020}. The `headline' number in climate science has for a long time been change in global mean temperature \citep{stocker_climate_2013,ipcc_global_2018,masson-delmotte_climate_2021}. While this is clearly a very important number as the primary metric of the impact that humanity is having on the climate, it is not a number that individuals can easy relate to due to the large scales involved and indirect nature of the associated impacts. On the other hand, extreme weather events are phenomena that are actually experienced in real time by individuals --- and regularly reported on by the media. Since extreme weather events can cause severe and direct socioeconomic impacts \citep{fouillet_excess_2006}, increases in their frequency would likely be a considerably more relatable and concerning consequence of climate change than the distant change in global mean quantities. Previous work has shown that extreme event attribution may be an exceptionally useful tool for climate change communication \citep{ettinger_whats_2021}, though can prove unhelpful if results are not clear and comprehensible for a general audience \citep[for example if different attribution studies regarding a single event appear to provide conflicting headline results][]{osaka_natural_2020}. A specific study investigating the experience-perception link of climate disasters in the context of Floridians that had experienced hurricane Irma found that this experience increased both their belief and concern in global warming \citep{bergquist_experiencing_2019}, though a more recent study looking at connections between local weather and climate change awareness in Germany did not find a link \citep{gartner_experiencing_2021}.

  There are also a number of more scientific motivating factors behind studying extreme event attribution. The first of these is the improved understanding of how climate change extreme weather events. By studying a single weather event in detail, we can gain a significant amount of knowledge about not only the processes that drove that event, but also about how those processes may have changed due to human influence on the climate. The nature of extreme events means that they are often driven by either unusual processes --- or combinations of processes --- that rarely occur. As such, they provide an opportunity to explore these rare drivers in detail and obtain a better understanding of their underlying physics. One example of this improved understanding has been the role of soil moisture on heatwaves \citep{fischer_soil_2007,fischer_contribution_2007,wehrli_identifying_2019}. % improve models (high benchmark of test) / climate projections - decide whether to include / adaptation planning / improve understanding of changing risks for eg. insurance / improved understanding of specific physical processes

  On a related note, this same granular look at a single event allows us to examine the performance of the dynamical models we use by ensuring that the processes and physics of the modelled climate matches those same processes in the real world \citep{sillmann_understanding_2017,philip_protocol_2020}. This is especially true for weather forecast models, which aim to be able to precisely reproduce the real world. Even in the case of climate models, extreme event attribution studies have identified key issues in the simulations produced by climate models such as poor representation of climate variability \citep{bellprat_towards_2019,leach_anthropogenic_2020}.

  The final reason I shall mention here --- though I discuss it further in the discussion section of this thesis \ref{discussion} --- is that how extreme events are changing in the present and will continue to change in the future is a question of vital importance for adaptation to climate change. Although the focus of attribution is typically understanding how extreme events have changed relative to a world without human influence on the climate, another side of a very similar coin is understanding how such influence will continue to change them into the future \citep{harrington_integrating_2022}. This projection of future extremes will be vital for effective and targeted adaptation planning given the severe damage such events can cause. In addition to policymakers, a large proportion of the industrial sector, and especially the financial sector, need to know how climate change is affecting the risk from extreme weather on a continual basis in the present day. For example, insurance and reinsurance companies base their risk capacity on historical loss estimates --- but in the case of many types of extreme weather event the baseline is shifting with climate change and historical data may not necessarily be representative of the present. This shifting baseline is something that extreme event attribution is well-placed to inform on.

\section{Answering the question}

  At this point, I have discussed the question that this thesis is primarily concerned with, and the reasons why it is of broad importance. Now, I shall describe and explore the ways in which previous studies have approached this question, in particular focusing on the probabilistic \citep[often ``conventional'',][]{stott_human_2004} and storyline \citep{hoerling_anatomy_2013} frameworks. 

  \subsection{Probabilistic attribution}

    Probabilistic attribution seeks, ultimately, to determine the change in probability of an extreme event arising due to some external driver. This was the approach to extreme event attribution proposed by \citet{allen_liability_2003} and first applied by \citet{stott_human_2004} to the 2003 European heatwave. They applied an optimal fingerprinting technique \citep{hasselmann_optimal_1993,hasselmann_multi-pattern_1997} to transient climate model simulations. They used five simulations, one set of four with all forcings included, plus one with natural forcings only; generating scaling factors of the correspondence between the modelled response and observed changes by regressing each set onto observed central European summer temperature. The scaling factors could then be used to determine the 1990s temperature anomalies attributable to all forcings combined and natural forcings alone. A third control run at a fixed, pre-industrial level of forcing was used to estimate internal variability corresponding likelihood functions for these temperature anomalies. They finally used a peak-over-threshold extreme value analysis of the same control run to determine the probability of exceeding the temperature of the pre-2003 hottest summer in worlds with and without climate change by adjusting the mean summer temperature to the estimated 1990s temperature anomalies both with and without anthropogenic forcing. These probabilities (and associated uncertainty) could be used to determine the likelihood function of the change in risk of the heatwave attributable to human influence. This fairly involved statistical approach (in particular, the necessity for the use of a control run to estimate uncertainty due to internal variability) has been largely replaced by the use of much larger single- or multiple- model ensembles.

    The next advance in probabilistic extreme event attribution came with \citet{van_oldenborgh_how_2007}, who developed a methodology for estimating the change in risk of an extreme event using observations alone. \citeauthor{van_oldenborgh_how_2007} took observed timeseries of autumn temperatures measured by the De Bilt weather station, and removed the climate change signal by regression onto low-pass filtered global mean surface temperature (a reasonable proxy for anthropogenic influence on the climate, given the small contributions from natural forcing). Using this detrended series and extreme value analysis, he then computed the return period of the exceptional warm 2006 autumn in Europe, and compared it to the return period estimated using the original series. This method was later extended to use the return period to compute the return period of the extreme for both the present-day and pre-industrial period by shifting the detrended series by the attributable trend computed in the regression \citep[eg.][]{philip_protocol_2020,leach_anthropogenic_2020}. In this way, the change in risk between pre-industrial and present climates can be estimated. It is worth noting that this methodology does not formally attribute and changes in risk to anthropogenic influence, since trends in local climate may be influenced by other factors, and no use is made of a counterfactual world without human influence on the climate (since no observations of such a world exist). This method can also be applied to transient simulations from climate models.

    The final advance that I shall highlight is from \citet{pall_anthropogenic_2011}, and was the first instance where specific fixed forcing (as opposed to transient) factual and counterfactual simulations were used. \citeauthor{pall_anthropogenic_2011} generated very large (2000+ member) atmosphere-only climate model ensembles of autumn 2000. One ensemble was driven using observed sea surface temperatures and sea ice, and corresponding atmospheric conditions (greenhouse gas, aerosol and ozone concentrations) for that time. The other four were driven using atmospheric conditions for the year 1900, and subtracting four estimates of attributable twentieth-century SST warming from the observed sea surface temperatures; the four estimates of attributable warming were derived from four different coupled climate model simulations. River runoff for England and Wales in the factual ensemble was compared to runoff in the four `naturalised' counterfactual ensembles to determine the difference in risk of exceeding the value actually observed in autumn 2000 in the different climates. In this case, the ensembles are sufficiently large that extreme value analysis was not required (the -- SST conditional -- return period could be calculated by simply counting the number of members that exceeded the observed threshold and dividing by the total ensemble size in each case). This methodology is attractive due to the clear and straightforward statistical analysis of the large ensembles (with no reliance on extreme value analysis or optimal fingerprinting techniques). However, it does generally require very large ensembles of the time period when the event took place (i.e. autumn 2000 in this case); and is conditional on the prescribed SST pattern (requiring either that anthropogenic influence isn't affecting interannual modes of SST variability, or that the extreme in question is independent of such modes).

    The `standard' approach to probabilistic attribution in the present draws upon each of these previous advances \citep{philip_protocol_2020}. Here I am taking the World Weather Attribution project (WWA) methodology as standard, since they are the most prolific group both in terms of number of events analysed and media coverage of their results \citep{van_oldenborgh_pathways_2021}, though other methodologies exist \citep{lewis_anthropogenic_2013}. Their approach involves:
    \begin{enumerate}
      \item Defining the event. Extreme events are, by the nature of the weather, exceptionally high dimensional and could be described in a practically unlimited number of ways (i.e. what variables to use? What spatial scale? What temporal scale?). However, to be able to analyse changes to such events, we must be able to define them quantitatively. The WWA attempts to select a metric that most closely corresponds to the impacts of the extreme event in question, taking in account what questions are being asked by the various stakeholders. For example, if the key impact of interest is heatwave-associated mortality, then peak 3-day moving average daily maximum temperatures may be selected due to their close connection to health impacts \citep{dippoliti_impact_2010}. Once a metric has been chosen by which to define the extreme event, they use a `class-based' framing considering all events of a similar magnitude, often by choosing the annual maximum values of the metric. This class-based framing results in a largely unconditional analysis, which I will discuss further below.
      \item Analysing trends in observed data following \citet{van_oldenborgh_how_2007}. This is typically done by fitting an appropriate distribution whose parameters shift or scale with low-pass filtered global mean surface temperature (GMST). The shifting or scaling depends upon the chosen metric and its observed or expected response to climate change. For example, the temperature based heatwave metrics that are the primary concern of this thesis are typically assumed to simply shift with global warming. From this GMST-covarying distribution, the return period is then computed for present-day and pre-industrial values of GMST. From these returns periods the change in risk of the observed extreme can be calculated. As with \citet{van_oldenborgh_how_2007}, these changes in risk are not strictly attributable to human influence on the climate due to the lack of a no-human counterfactual.
      \item Analysing simulations from as many models as possible. Transient model simulations are analysed in an identical way to observations. Fixed forcing simulations are analysed following \citet{pall_anthropogenic_2011}. Only models which are able to closely represent the observed climate are considered -- evaluated on the basis of the trends and distribution parameters implied by the model data.
      \item Synthesising these various strands of evidence. The aim behind using as many lines of evidence as possible, combining statistical and numerical models, is to try and determine the most robust conclusion possible within the context of the associated uncertainties. 
    \end{enumerate}
    This `standard' approach has been refined over the past decade by the WWA team \citep{van_oldenborgh_pathways_2021}, learning through application to a wide range of locations and types of extreme. The widespread recognition and understanding of extreme event attribution by the general public is due, in no small part, to this approach and how rapidly the WWA team have been able to apply it to extreme events in the past few years. Their rapid response has meant that they are able to answer the questions people ask in the aftermath of such events when they are actually asking them -- rather than following a lengthy peer-review process. However, this standard probabilistic approach to attribution is not without issues of its own -- hence this thesis -- which I shall now discuss.

    The first issue arises due to the unconditional use of climate models. By their nature, extreme events are typically produced by exceptional physical processes, or combinations of processes. The type of models used in extreme event attribution are typically coarse (O(100 km)), and may well not be able to physically represent all the processes involved in the production of specific extreme events even if they can accurately represent the average climate \citep{sillmann_understanding_2017,trenberth_attribution_2015,demory_role_2014}. Such models still have serious known biases relevant to the simulation of extremes, including poor representation of Euro-Atlantic blocking \citep{schiemann_resolution_2017,dorrington_how_2021}, which is a key synoptic driver of heatwaves over the continent. These biases become even more important when considering not only the models' ability to simulate the present climate, but also their response to external forcings such as anthropogenic climate change \citep{palmer_nonlinear_1999,palmer_simple_2018}. The use of biased models can lead to potentially incorrect quantitative attribution statements \citep{bellprat_attribution_2016,bellprat_towards_2019}. 
    
    The second key issue derives from the treatment of individual extremes as one of an event class. For example, in the conventional probabilistic approach to attributing a particular heatwave, one might consider all the previous annual maximum temperatures (e.g. in order to apply extreme value analysis as discussed above). However, the particular heatwave in question might have arisen from a very different -- possibly unique in the context of the relatively short historical record -- set of meteorological circumstances and physical drivers compared to all the previous heatwaves. This not only renders such extreme value analysis as is often performed potentially inappropriate (as the heatwave in question is drawn from a different underlying distribution to the others), but also any estimated climate change responses. What if the combination of the particular processes involved in producing the heatwave in question responds fundamentally differently to the processes that have generated past heatwaves?
    
    The final issue I shall discuss is the one that has been most often commented on in previous work: the risk of type II errors \citep{shepherd_common_2016,trenberth_attribution_2015}. This risk arises because some aspects of the climate system response to external forcing are much more certain and well-understood than others. For example, the thermodynamic aspects of climate change are broadly very well understood and certain: rising greenhouse gas concentrations lead to a thicker troposphere, thus raising surface temperatures and increasing the moisture capacity of the troposphere. On the other hand, the dynamic aspects of climate change are considerably less certain, with models often disagreeing over the direction of changes in atmospheric dynamics \citep{masato_winter_2013}. At this point, I note that this is not an entirely independent issue to the first issue discussed since much of this uncertainty arises due to model biases and imperfections. Since extreme events are often driven by a combination of both thermodynamic and dynamic processes, the very certain thermodynamic climate change impacts can be masked to some extent by the much less certain dynamic climate change impacts. This uncertainty can lead to falsely asserting that there is no overall impact -- a type II error. \citet{trenberth_attribution_2015} argue that it is better to focus on the aspects of the event that are well understood, for example by conditioning analyses upon the large scale circulation of the event in question, thus removing the potential very uncertain dynamic aspects of climate change. This suggestion was extended and discussed at length by \citet{shepherd_common_2016}, and has become the basis for the new kid on the block in extreme weather attribution: the storyline approach.

  \subsection{Attribution through storylines}

    The storyline approach \citep[or `Boulder' approach,][]{otto_attribution_2017} aims to determine the contribution of various causal factors that to the extreme event, and considers how anthropogenic climate change has affected those factors (and thus the extreme) in a deterministic manner. This approach was first applied by \citet{hoerling_anatomy_2013} to the 2011 summer combined Texas heatwave and drought. They used a variety of simulations, including atmosphere-only and coupled climate model runs, and seasonal forecasts. They examined the influence of rainfall deficit in the months preceding the heatwave, SST influence on the drought, and the overall predictability of the extreme at the start of May. As such, their intended goal was much broader than just assessing the anthropogenic contribution to the heatwave, aiming to advance the overall predictability of such events by examining and understanding many drivers, both human and natural. This approach of trying to disentangle and quantifying the contributions of many different drivers individually laid the basis for what most would understand as the storyline approach to extreme event attribution.

    The next key text that I discuss is a perspective, \citetitle{trenberth_attribution_2015} by \citet{trenberth_attribution_2015}. This highlighted the potential for conventional probabilistic attribution to suffer from type II errors due to the large component of extreme weather events that arises from internal variability of the climate, thus masking any clear anthropogenic signals. The authors suggest that attribution studies should focus on the drivers of extreme weather and associated impacts whose response to climate change is determinable given the present state of knowledge. For instance, following a tropical cyclone event, rather than focussing on the probabilistic question of whether the unconditional probability of an event has changed, which would require assessing potential changes in large scale circulation patterns; they suggest focussing on aspects of the event whose response to climate change is well-understood and physically motivated, such as the increase in sea surface temperatures and available moisture leading to a deeper storm and more intense precipitation. They distil their approach down to answering the conditional question: `given the change in atmospheric circulation that brought about the event, how did climate change alter its impacts?'. In general, this is equivalent to determining how the known changes in the climate system's thermodynamic state have affected the event in question. Of particular interest in relation to this thesis is their suggestion that `one needs to be able to simulate the event in question (perhaps with short-term forecasts...)', since this is a strong motivating factor for the overarching approach taken here.

    A key figure in the recent advancement of the storyline approach is Theodore Shepherd. \citet{shepherd_common_2016} discusses the differences in framing between the probabilistic and storyline approaches to extreme event attribution and demonstrates that they can be cast into a single framework based on conditional probabilities. He also discusses similar issues with probabilistic attribution to those I have outlined above, including the class-based framing and climate model deficiencies. In \citet{shepherd_storylines_2018}, the authors present an argument for the use of storylines more generally to understand and communicate the physical impacts of climate change in both present events and future projections. They summarise their argument in four reasons: 
    \begin{itemize}
      \item improving risk awareness due to the episodic (experience-based) nature of storylines as opposed to the semantic (knowledge-based) nature of conventional probabilistic approaches, given humans are more likely to respond to episodic rather than semantic information;
      \item strengthening decision-making by allowing decision-makers to work backwards from a particular vulnerability incorporating climate change information with other factors, or to develop stress-tests based on (modified) historical events;
      \item physically motivated partitioning of uncertainty - explicitly drawing the distinction between the more certain thermodynamic aspects and the less certain dynamic aspects of climate change. The basis for this argument is set out in \citet{trenberth_attribution_2015} as discussed above;
      \item exploring the boundaries of plausible risks by combining information from climate model simulations with scientific understanding of the climate system rather than simply relying on quantitative results from climate model simulations. In this way, they suggest that the storyline approach could guard against surprise impacts from processes not well simulated or whose uncertainty is not well reflected by the current range of climate model projections, using local-scale precipitation events as a key example.
    \end{itemize}

    Now that I have discussed the formal basis for the storyline approach, here I present a selection of recent studies that apply it to various extreme weather events. \citet{van_garderen_methodology_2021} introduces a methodology for extreme event attribution based on constraining the large-scale circulation in a climate model to that observed in reality. They employed global spectral nudging to push the mid-troposphere to upper-stratosphere (the `free' atmosphere) towards reanalysis data, allowing the thermodynamic fields they were interested in (temperature and moisture) to respond to the imposed circulation. Their aim was to `constrain the model as little as possible... while still achieving an effective control of the large-scale weather situation'. The produced factual and counterfactual nudged simulations of the 2003 and 2010 heatwaves in Europe. The factual simulations were based on using present-day observed reanalysis, SST, and greenhouse gas concentration data; while in the counterfactual simulations, the SSTs were perturbed based on estimated changes since the pre-industrial, and GHG concentrations were changed to their pre-industrial levels. They compared their factual and counterfactual simulations to find that (domain-averaged) anthropogenic contributions to the 2003 and 2010 heatwaves were 0.6 °C and 2 °C respectively. This approach was further developed by \citet{benitez_july_2022}. Unlike \citet{van_garderen_methodology_2021}, who used prescribed SST patterns and an atmosphere-only model, \citeauthor{benitez_july_2022} used a coupled climate model, in order to permit ocean atmosphere interactions that may be associated with the particular event of interest. To do this, they branched from free-running simulations of a coupled climate model at particular levels of global warming (pre-industrial, present-day, 2 °C and 4 °C). They allow for one year of spin-up time to allow slowly responding fields to develop prior to the event of interest. The event that they focussed on was the July 2019 heatwave in Germany, which they assessed had been made 2 °C warmer since pre-industrial times. Overall, their analysis could be considered an archetypal storyline approach to a heat extreme, and provides a useful contrast to the approach developed over the course of this thesis. The final study I discuss here, \citet{schaller_role_2020}, does not concern a heatwave, but a flood event. However, it is notable due to the authors' comprehensive analysis of a single event and focus on using operational systems as they are familiar to the stakeholders who would benefit most from the information provided by the study. They combined high-resolution climate model simulations and regional model downscaling of the same simulations with hydrological models to provide physically plausible storylines of present-day and future flooding associated with atmospheric river events in western Norway. 
    
    The storyline approach is able to address several of the issues with probabilistic event attribution raised above, and is clearly able to provide extremely valuable information about the changes in present extremes due to human influence on the climate, as well as potential future changes. However, it is unable to quantitatively estimate the changes in probability of such extremes --- the aim of the probabilistic approach. I argue that provision of this probabilistic information remains important for public communication of climate change risks and potential future litigation. In this thesis I primarily explore an approach that leverages numerical weather prediction models in order to synthesise between the storyline and probabilistic frameworks, alleviating many of the issues of the latter I discussed above whilst maintaining the ability to provide meaningful probabilistic information.

  \subsection{A forecast-based approach}

    Although often considered distinct fields of research, weather prediction and climate projection are ultimately seeking to understand the same physical system --- just on very different timescales. As such, the models used in each field share many similarities and components. For example, the Met Office Hadley Center uses a unified framework for weather and climate modelling based on their `Unified Model' \citep{brown_unified_2012}. Differences between the two begin to arise when considering the questions that each field has traditionally aimed to answer. Weather prediction tries to determine how the atmosphere will evolve over typical timescales of hours to months. Due to the chaotic nature of the atmosphere \citep{lorenz_deterministic_1963}, a key part of weather forecasting is estimating its initial state as accurately as possible --- weather prediction is an initial value problem. Since many weather phenomena of interest are highly localized (for example, convective storms in the UK), providing information on a similarly local level is important, hence weather prediction models are typically run at very high resolutions. On the other hand, climate simulation and projection does not try to estimate what the precise state of the earth system will be in the decades to come, but to produce realisations that lie within the space of all possible states. By averaging such realisations over many years or different simulations (ensembles), you can build up a complete picture of this space (e.g. `Weather is what you get, climate is what you expect'). An important aspect of climate projection is that where this space lies depends strongly on an external forcing you apply to the system --- climate projection is a boundary-value problem. Due to the very long simulations and traditionally larger spatio-temporal scales of interest, climate models are typically run at much lower resolutions. This difference in typical resolution provides the first justification for why forecast-based approaches to extreme event attribution: using these higher-resolution forecast models may resolve many of the known issues and biases in climate models, especially in the context of extreme weather \citep{sillmann_understanding_2017,schiemann_resolution_2017,demory_role_2014}.

    % \clearpage
    \begin{figure}[h]
      \centering
      \includegraphics[width=\textwidth]{{FigI.1}.pdf}
      \caption[Comparing typical horizontal resolutions of weather and climate models]{\textbf{Comparing typical horizontal resolutions of weather and climate models.} The left panel shows wind gusts at 10 m on 2021-10-01 00:00:00 over Europe, based on reanalysis \citep{hersbach_era5_2020}, at the 18 km resolution (Tco639) of the ECMWF IFS ensemble prediction system. The right  panel shows the same image, but at the 130 km resolution (N96) of the HadGEM3-GC3.1-LL climate model. This is arguably a generous comparison to the climate model, since this figure shows the same field at different resolutions, while it is conceivable that some features would not be able to be simulated at all in the lower resolution climate model.}
      \label{figI.1}
    \end{figure}
    % \clearpage

    One way in which weather prediction and climate simulation or projection fundamentally differ is in the extent to which they can be validated. Forecasts from weather models are able to be validated against reality (once reality has come to pass), but this is not possible with climate models since they do not aim to precisely replicate reality (only produce a possible realisation). This intrinsic validation of weather forecast models has led to a significant amount of work in which they are used to examine the sources of biases and errors in climate models \citep{williams_transpose-amip_2013}, or are used to calibrate climate projections \citep{palmer_toward_2008,matsueda_calibrating_2016}. This is the philosophy behind seamless prediction: that in a unified weather to climate prediction system, insights gained from the validatable initial-value problem can be transferred to improve the confidence in answers to the boundary-value problem. In the context of extreme event attribution, this idea has been discussed and examined by \citet{weisheimer_atmospheric_2017,palmer_simple_2018,lott_evaluating_2016,bellprat_attribution_2016,bellprat_towards_2019}. These studies have typically focussed on using the known `reliability' of weather prediction systems to calibrate the outcome of probabilistic extreme event attribution. Here I use reliability in the statistical sense, meaning that the forecast probabilities of a particular event match up with the observed frequency at which that event occurs \citep{weisheimer_reliability_2014}. Although such calibration is vital for improving the trustworthiness of existing probabilistic approaches to attribution based on climate models, I argue that even more benefit would be gained by using weather prediction systems for attribution directly. The key advantage of using a weather prediction system directly is that you can know with certainty if the model is capable of simulating the event you are interested in. If a weather forecast model is able to successfully predict an extreme event, then you can have considerable confidence that the model is able to represent all the important physical processes involved in the development of the event. You can also have confidence in the response of these --- well represented in the model --- processes to external forcing. This translates to increased confidence in any attribution statements that arise from analysis of the model. The same is not true of climate model based analyses. It would be incredibly difficult and involved to test whether a climate model is even able to simulate a specific extreme weather event and all the associated processes and drivers as they evolved in reality. In the conventional approach to probabilistic attribution, the validation of models is purely statistical \citep{philip_protocol_2020}.

    This intrinsic validation of using weather forecast models directly is related to another motivating factor. Given a successful prediction, weather forecast ensembles produce realisations of the specific event of interest. This means that an attribution analysis based on such an ensemble can claim to be attributing the individual weather extreme, and therefore answering the question of how human influence has affected the probability of that precise event. This is in contrast to conventional probabilistic approaches based on climate models, which frame attribution analyses in terms of the event as one occurrence of a particular class of events (e.g. the 2019 heatwave in Europe might be abstracted into the class of annual maximum temperature events). As discussed above, this means that the physical processes behind the event, that may be unique to that specific event, are not taken into account. The question answered by conventional probabilistic attribution is somewhat different and less specific. For the 2019 heatwave example, this question would be: how has human influence affected the probability of the peak annual temperature exceeding the value experienced during the 2019 heatwave \citep{vautard_human_2020}? The more specific nature of the question answered by a forecast-based approach brings it closer to the episodic nature of the storyline approach. It may also be useful when considering the legal contexts in which attribution studies are used \citep{lloyd_climate_2021-1}.

    \subsubsection{Operationalising attribution}

      One of the traditional `problems' with extreme event attribution has been that the timescales on which scientific research is published are very different from the timescales on which the public and other stakeholders are (increasingly) most interested in. When an extreme weather event occurs, interest in the media is usually higest in the days immediately following the event, while a scientific analysis would typically take months to be published. A second problem that is not unique to extreme event attribution is that studies tend to be focussed on the same regions that the research is conducted in. This leads to a bias in the coverage of such studies, with far more studies looking at events in the global North, especially Europe. The World Weather Attribution Project has sought to alleviate this bias by engaging with local researchers \citep{van_oldenborgh_pathways_2021}. A proposed solution to these issues is an operational attribution system, that could provide rapid results when extreme events occur using a well-established and consistent approach \citep{national_academies_of_sciences_engineering_and_medicine_attribution_2016}.

      There are a number of practical reasons why a forecast-based approach is an attractive proposition as the basis for an operational attribution system (in addition to the science-focused arguments presented above). Firstly, weather models are run routinely on sub-daily cycles. If a methodology were created that allowed an operational weather forecast system to be easily switched to run in a counterfactual pre-industrial mode, then such simulations could be generated extremely rapidly. With sufficient interest and funding in the future, perhaps such counterfactual simulations could be run regularly alongside the routine operational forecasts. Given the experience and expertise of weather forecast centres in the operational application of the science of weather prediction, it seems clear that they could add considerable value to the efforts towards operational attribution \citep{wehner_operational_2022} --- but especially if consistent models are used between the two. A final point to consider is that weather forecast model output is already used widely in hazard warning systems. The findings of extreme event attribution studies are of considerably relevance to such hazard warning systems, and other users in this space, since they can quantify how the risk of such hazards has changed due to climate change, and how the risk might change further in the future. Using tools that these users are already familiar with, in this case weather forecast models, will maximize the utility and impact of further studies \citep{schaller_role_2020}.

\section{Conceptualising different approaches}

  % use the dynamicist's favourite "toy": Lorenz63 / 90?
  % demonstrate "storyline / DADA", conventional attribution
  % propose forecast-based attribution
  \blindtext

\section{The meteorology of heatwaves}

  Thus far, I have discussed attribution of extreme weather in a general context --- and the ideas and approaches above can be applied to any kind of extreme event. However, much of this thesis concerns heatwaves. There are several reasons for this. Firstly, heatwaves can have severe and wide-ranging socioeconomic and ecological impacts \citep{mitchell_attributing_2016,lo_estimating_2022,schauberger_consistent_2017,davis_arabica-like_2021}. Their broad response to climate change, and the underlying physical basis is well understood. Heatwaves have been the most common class of extreme weather event analysed in past attribution studies, which provides an extensive collection of literature for comparison over the course of this thesis \citep{noauthor_mapped_2021}. Due to the focus on heatwaves here, in this section I will review the typical meteorology of heatwaves, and the physical basis for their expected response to climate change. I will arrange this review in approximate order of proximity (starting with planetary- and synoptic-scale drivers, and ending at the mesoscale), and focus on the meteorology of mid-latitude heatwaves, since this is where all the case studies covered in this thesis occurred.

  I will begin by discussing features that would be classed as `dynamical' drivers of heatwaves. One widely acknowledged feature is atmospheric blocking \citep{garriott_long-range_1904,kautz_atmospheric_2022}. Blocking systems cover a wide range of atmospheric structures that are characterised by their persistence and stationarity. Although there is no unique accepted definition of a block, a rapid change from zonal to meridional tropospheric flow would be considered essential by many. In summer, over the regions where blocking generally occurs, zonal flow brings cooler oceanic air, and its interruption therefore leads to increased temperatures. Blocks often feature large anticyclonic anomalies, which can lead to subsidence under high pressure, adiabatically heating air as it falls in a so-called `heat dome'. Anticyclonic anomalies also reduce cloud cover and precipitation, increasing temperatures in summer even further. Their persistence enhances these effects by allowing them to build up over one or more weeks. These processes that lead to anomalously high summer temperatures are why blocking has been a key contributor to a number of noteworthy heatwaves \citep{woollings_blocking_2018}. Although I will not discuss the precursors of blocking in depth, I note that some work has shown that blocks can arise as a result of tropical-origin wave-trains \citep{schneidereit_large-scale_2012,greatbatch_tropical_2015}, and can be preconditioned by specific regional sea surface temperature and sea ice patterns \citep{di_capua_drivers_2021,wang_summer_2020}. Blocking systems are projected to gradually decrease in frequency in the mid-latitude with global warming, though considerable uncertainty is associated with this change due to its sensitivity to the specific method used, lack of theoretical support, and poor representation in climate models \citep{woollings_blocking_2018}. However, alongside this decrease in blocking frequency, some work has shown that block size might be expected to increase with global warming \citep{nabizadeh_size_2019}, which may change the spatio-temporal characteristics of weather extremes associated with these systems.

  Next, I shall review how synoptic-scale heat is actually generated from a physical, rather than a meteorological perspective. I shall examine three different processes by which heating occurs: advective, adiabatic and diabatic. Although these are clearly not entirely distinct in the atmosphere, I believe that treating each of these processes in turn will aid the clarity of this discussion. Advective heating occurs when regional temperatures increase as a result of warm air transport into the region. In the mid-latitudes, this warmer air is typically transported from the tropics. Advective heating is dynamical in origin, and may arise as a result of a `ridged' tropospheric flow, as was the case in the exceptionally warm February temperatures observed over the UK in February 2019 \citep{young_record-breaking_2020,leach_forecast-based_2021}. Adiabatic heating occurs when an air mass descends, compressing as the pressure increases, and thereby increasing its internal energy and temperature. This is the type of heating directly associated with anticyclonic blocking systems (or `heat domes'), in which high pressure causes air parcels to subside, increasing their temperature as they approach the surface. Diabatic heating is a broad class of many relevant processes, including condensational (latent) heating and radiative heating from absorption of infrared light, primarily by atmospheric water vapour, carbon dioxide and ozone. Rather than focussing on the broad atmospheric response to greenhouse gas emissions that arises as a result of these processes, which has been well-understood for a long time \citep{arrhenius_influence_1896}, here I shall briefly discuss feedbacks arising from these processes that may enhance heat extremes in particular due to human influence on the climate. % clausius clapeyron -> increased wv radiative heating -> warmer lower atmosphere -> more water vapour possible in parcel -> hotter feedback loop (under specific anticyclonic conditions preventing convection / condensation into clouds)?

  Finally, I consider meso- and local scale meteorological processes that control the intensity of heat extremes. As mentioned above, the anticyclonic systems often associated with heat extremes result in low cloud cover, due to the lack of condensation in subsiding air. This low cloud cover results in increased solar radiation at and near the surface. This increase in radiation results in diabatic heating, enhancing the impact of the (adiabatic) heating through subsidence. One way in which this radiative heating can be moderated is through the presence of soil moisture, which results in latent heating, thus reducing the energy transfer contributing directly to increased air temperatures. As a result, soil-moisture feedbacks are a key physical component of heat extremes that have been studied extensively \citep{horton_review_2016,wehrli_identifying_2019,zeppetello_physics_2022,vogel_varying_2018,fischer_contribution_2007,fischer_soil_2007,miralles_mega-heatwave_2014,sousa_distinct_2020}. \citet{vogel_varying_2018} partitioned the effect of soil moisture into soil-temperature, soil-precipitation and soil-radiation feedbacks to allow for a clear explanation of the underlying processes involved. Soil-temperature feedbacks occur when dry soils lead to reduced latent heating, thus increasing temperatures and latex heat flux, further drying out the soil. Soil-precipitation feedbacks dampen the temperature response when an increase in soil moisture leads to increased latex heat flux, resulting in increased cloud cover and precipitation, thus further increasing soil moisture. Soil-radiation feedbacks are similar, in that reduced soil moisture results in reduced latent heat flux, thus reducing cloud cover and enhancing incoming solar radiation, thus further drying the soil. \citet{fischer_soil_2007} additionally demonstrated for the case of the 2003 heatwave over Central Europe that reduced soil moisture enhanced positive geopotential height anomalies, thus increasing the persistence and strength of the associated anticyclonic circulation, further reducing soil moisture. Human influence is understood to be reducing soil moisture on a global scale \citep{gu_attribution_2019}, though the confidence in this impact (and its sign) varies considerably across regions. \citet{marvel_twentieth-century_2019} used detection and attribution techniques to find an emerging greenhouse gas signal forcing the drying trends over North America and Eurasia. Confident detection and attribution of these signals is impacted by the influence of anthropogenic aerosol emissions on regional hydroclimate, particularly during the mid to late twentieth century. Aerosol influence on such trends has a distinct spatial pattern that masks the greenhouse gas pattern and associated signal \citep{bonfils_human_2020}. Overall, however, reductions in soil moisture due to anthropogenic greenhouse gas emission are expected to enhance human influence on extreme heatwaves through the mechanisms discussed here.

\section{What to expect in this thesis}

  This chapter has provided a review of the attribution of extreme weather events to human influence on the climate. I have introduced the question that this fields aims to answer, motivated why it is an important question, and discussed several approaches to answering it. I finally provided an overview of the physical processes involved in generating the kinds of heatwaves that will be the focus of much of this thesis. This final section will summarise the content within the remaining chapters. One consistent feature of the science chapters (\ref{ch2}--\ref{ch5}) is that they begin with a `Chapter open' and end with a `Chapter close' section, written after I had completed them all. In the `Chapter open' sections I discuss the aims of the chapter within the wider context of my whole PhD project, and what the intended outcome of the chapter is. In the `Chapter close' sections I consider how the outcome of the chapter fits within the whole project, what questions are still outstanding following the chapter, and how these questions influence the rest of the thesis.

  In Chapter \ref{ch2}, I carry out a conventional probabilistic attribution study, following the methods I have described above. This analysis uses the hot summer over Europe in 2018 as its case study. The original motivation for this chapter was that following the event, groups from the UK Met Office and WWA released attribution statements, coming to apparently conflicting results that differed by an order of magnitude. This study aimed to resolve the discrepancies between these two estimates by examining the influence of how the event is defined, in terms of its temporal and spatial scale, upon the quantitative attribution results. Within the context of this thesis, this chapter provides a more in-depth introduction to the techniques and presentation of results within the conventional probabilistic attribution framework.
  % exploratory conventional attribution study

  Chapter \ref{ch3} introduces the approach that is the primary outcome of this thesis: extreme weather attribution using counterfactual weather forecasts of specific events. In this chapter, I look at the case study of the exceptional warmth experienced over much of Europe at the end of February 2019, and confine the scope of the attribution to examining the direct radiative influence of increased CO$_2$ levels over pre-industrial levels. I start from the successful operational ECMWF forecast ensemble system, re-run the ensemble with pre-industrial CO$_2$ levels, and look at the impact this boundary condition perturbation has on the extreme heat. With this partial attribution, I show how a forecast-based approach might be carried out in practice. I discuss how our forecast-based approach differs in terms of the question it is answering to conventional probabilistic approaches, alongside some of the arguments for and against using such an approach.
  % first step to forecast-based attribution - partial attribution

  In chapter \ref{ch4}, I develop the partial attribution study in chapter \ref{ch3} by additionally taking anthropogenic influence on ocean heat content into account. I do this by perturbing the initial ocean (and sea-ice) state of the weather model in such as way as to remove the human fingerprint from the forecast initial conditions. By perturbing both the initial and boundary conditions of the weather model, I can more completely estimate the human contribution to an individual weather event. The event that I study in this chapter is the 2021 Pacific Northwest Heatwave, an unprecedented extreme that generated significant attention from the media when it occurred. One reason why this event is of particular interest is because the conventional probabilistic approaches appear to break down when faced with such an exceptional outlier in the context of the historical record. I show that the forecast-based approach taken here represents a robust and appropriate alternative to these conventional approaches.
  % "full" forecast-based attribution

  The final science chapter \ref{ch5} focuses on projections of future climate, a field which has strong links to attribution. However, unlike the other chapters in which I attempt to increase the specificity of attribution frameworks, in this chapter I explore questions over how to estimate and sample the full space of uncertainty surrounding future weather extremes. I use a novel approach to generate very large ensembles of extreme winters in an atmosphere-only model, comparing these ensembles to relatively smaller ensembles from a coupled model. I show that this novel approach represents a very efficient methodology for the provision of very extreme winters over the UK, which could be of considerable value for climate change adaptation planning. I consider the question of how the forecast-based approaches to attribution could be appropriated to provide physically-consistent samples of specific high-impact extreme weather events as if they occurred in a future climate. This forecast-based approach to climate projections would provide very specific information, in contrast to the general exploration of uncertainty allowed by the work in this chapter. Both approaches could, however, provide exceptionally useful information surrounding the risk from future extreme weather events.
  % tangent study about climate change projections + link to forecast based approaches

  Chapter \ref{discussion} provides a discussion of this thesis. Beginning with a summary of the science I have presented, I move onto placing it within the body of previous work on the subject and assessing its novelty and utility. I consider some of the limitations within the studies and approaches taken here, and suggest ways in which they might be overcome with future work. There are a number of ways in which this work could progress further in the future, or could be used outside of attribution. I consider these possible directions and applications before giving my concluding thoughts.
  % conclusion discussing current limitations + future directions